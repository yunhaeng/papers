{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "3joTKBUrD_tu"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Attention & Transformer\n",
        "**어텐션(Attention)**과 **트랜스포머(Transformer)**는 자연어 처리 분야에서 굉장히 활발하게 사용되고 있는 모델입니다.</br>\n",
        "두 모델을 사용하면 영어를 한국어로 바꾸어주는 번역기나, 질문에 대한 답을 제공하는 챗봇 등을 어렵지 않게 만들어 낼 수 있습니다.\n",
        "\n",
        "해당 노트에서는 **어텐션과 트랜스포머 모델의 구조와 원리**에 대해 알아보고 간단한 **한국어 챗봇을 구현**해보도록 하겠습니다."
      ],
      "metadata": {
        "id": "9yIGzVvB7C4r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 필요 라이브러리"
      ],
      "metadata": {
        "id": "3joTKBUrD_tu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import re\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Attention\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "162OglRYD-Be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "어텐션과 트랜스포머 모델에 대해 알아보기 전에 문장 생성의 가장 기본적인 모델인 **Seq2Seq**부터 알아보도록 하겠습니다.\n",
        "\n",
        "## **1. Seq2Seq**\n",
        "시퀀스 투 시퀀스(Sequence-to-Sequence) 모델은 **입력된 도메인의 정보를 바탕으로 다른 도메인의 결과를 출력하는 모델**입니다.</br>\n",
        "모델이 가지고 있는 특징때문에 챗봇, 기계번역 등 굉장히 다양한 분야에서 사용되고 있습니다."
      ],
      "metadata": {
        "id": "28bBfOn_7mxO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1-1. 모델 구조**\n",
        "Seq2Seq 모델은 크게 **인코더(Encoder), 컨텍스트 벡터(Context vector),  디코더(Decoder)** 3가지로 이루어져 있습니다.</br>\n",
        "\n",
        "<img src=\"https://i.imgur.com/yCppt05.png\" alt=\"seq2seq_6\" width=\"800\" />"
      ],
      "metadata": {
        "id": "8mwQ9cQv9rD6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1) 인코더**\n",
        "- 인코더는 **입력 정보의 특징을 추출하는 모듈**입니다.</br>\n",
        "- 인코더 내부에 있는 RNN, LSTM, GRU와 같은 순환 신경망 모델을 통해 입력 데이터의 특징을 추출하고,</br> 하나의 컨텍스트 벡터로 만들어 디코더에 전달합니다.</br>\n",
        "- [입력값(Input)] : 번역하고자 하는 문장(기계번역) 또는 질문(챗봇) 등을 입력으로 받습니다.\n",
        "- [출력값(Output)] : 입력 데이터에서 추출한 특징을 담은 컨텍스트 벡터를 생성합니다.</br>\n",
        "<img src=\"https://i.imgur.com/BofLfSi.png\" alt=\"seq2seq_6\" width=\"400\" />"
      ],
      "metadata": {
        "id": "Mhon88wOA3K2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2) 디코더**\n",
        "- 디코더는 입력 정보를 토대로 **새로운 정보를 생성하는 모듈**입니다.</br>\n",
        "- [입력값(Input)] : 인코더에서 생성한 컨텍스트 벡터와 현재 생성하고 있는 출력 정보 </br>\n",
        "- [출력값(Output)] : 순차적으로 단어 생성 </br>\n",
        "<img src=\"https://i.imgur.com/YTTVW3e.png\" alt=\"seq2seq_6\" width=\"400\" />"
      ],
      "metadata": {
        "id": "geMMOZWCBViF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3) 컨텍스트 벡터**\n",
        "- **입력 데이터의 특징을 모두 담고있는 벡터.**</br>\n",
        "- Seq2seq에서는 인코더의 가장 마지막 LSTM(혹은 RNN)에서 생성한 **hidden state 벡터 및 cell state 벡터**가 된다. </br>\n",
        "- 디코더의 첫 LSTM에 들어가는 hidden state 벡터 및 cell state 벡터"
      ],
      "metadata": {
        "id": "lA4lDuAPHBQw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1-2. 모델 구현**\n",
        "인코더와 디코더를 구현하고 하나로 합쳐 Seq2Seq 모델을 만들어보도록 하겠습니다."
      ],
      "metadata": {
        "id": "tP4mhsrVKNaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, units, vocab_size, embedding_dim, time_steps):\n",
        "        '''\n",
        "        Seq2Seq의 인코더입니다.\n",
        "\n",
        "        Args:\n",
        "            units (int) : 인코더 내부 lstm의 노드 수.\n",
        "            vocab_size (int) : 임베딩 행렬의 단어 수. 없는 단어가 있으면 oov가 발생할 수 있습니다.\n",
        "                -> 훈련하려는 문장의 단어는 모두 포함하고 있는 것이 좋다!\n",
        "            embedding_dim (int) : 임베딩 차원 수. 복잡할수록 좋을 수도 있고, 아닐 수도 있습니다. 차원이 크면 보통 표현력이 좋다. but 용량이 커져서 안좋을 수 있다.\n",
        "            time_steps (int) : 문장 토큰의 수. ex) 안녕하세요 조윤행입니다. -> 안녕하세요/ 조윤행/ 입니다/ -> 토큰 수 : 3개\n",
        "        '''\n",
        "        super().__init__()\n",
        "        self.embedding = Embedding(vocab_size,\n",
        "                                   embedding_dim,\n",
        "                                   input_length=time_steps)\n",
        "        self.dropout = Dropout(0.2)\n",
        "        self.lstm = LSTM(units, return_state=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.embedding(inputs)\n",
        "        x = self.dropout(x)\n",
        "        x, hidden_state, cell_state = self.lstm(x)\n",
        "        return [hidden_state, cell_state]"
      ],
      "metadata": {
        "id": "wAHQVGAzCG9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, units, vocab_size, embedding_dim, time_steps):\n",
        "        '''\n",
        "        Seq2Seq의 디코더입니다.\n",
        "\n",
        "        Args:\n",
        "            units (int) : 디코더 내부 lstm의 노드 수.\n",
        "            vocab_size (int) : 임베딩 행렬의 단어 수. 없는 단어가 있으면 oov가 발생할 수 있습니다.\n",
        "                -> 훈련하려는 문장의 단어는 모두 포함하고 있는 것이 좋다!\n",
        "            embedding_dim (int) : 임베딩 차원 수. 복잡할수록 좋을 수도 있고, 아닐 수도 있습니다. 차원이 크면 보통 표현력이 좋다. but 용량이 커져서 안좋을 수 있다.\n",
        "            time_steps (int) : 문장 토큰의 수. 디코더에서는 최대로 생성할 수 있는 문장의 길이가 됩니다.\n",
        "        '''\n",
        "        super().__init__()\n",
        "        self.embedding = Embedding(vocab_size,\n",
        "                                   embedding_dim,\n",
        "                                   input_length=time_steps)\n",
        "        self.dropout = Dropout(0.2)\n",
        "        self.lstm = LSTM(units,\n",
        "                         return_state=True,\n",
        "                         return_sequences=True)\n",
        "        self.dense = Dense(vocab_size, activation='softmax')\n",
        "\n",
        "    def call(self, inputs, initial_state):\n",
        "        x = self.embedding(inputs)\n",
        "        x = self.dropout(x)\n",
        "        x, hidden_state, cell_state = self.lstm(x, initial_state=initial_state)\n",
        "        x = self.dense(x)\n",
        "        return x, hidden_state, cell_state"
      ],
      "metadata": {
        "id": "eap6Gz62Gpcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2seq(tf.keras.Model):\n",
        "    def __init__(self, units, vocab_size, embedding_dim, time_steps, start_token, end_token):\n",
        "        \"\"\"\n",
        "        Seq2Seq 모델입니다. 인코더와 디코더를 선언합니다.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.start_token = start_token\n",
        "        self.end_token = end_token\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        self.encoder = Encoder(units, vocab_size, embedding_dim, time_steps)\n",
        "        self.decoder = Decoder(units, vocab_size, embedding_dim, time_steps)\n",
        "\n",
        "    def call(self, inputs, training=True):\n",
        "        \"\"\"\n",
        "        선언한 인코더와 디코더를 하나로 연결하여 Seq2Sqe 파이프라인을 구현합니다.\n",
        "\n",
        "        Args:\n",
        "            inputs : 문장의 단어 인덱스로 이루어진 데이터.\n",
        "            training : True인 경우 교사강요를 사용하여 디코더의 입력값에 정답을 넣어주며, False인 경우 디코더의 출력 데이터를 입력으로 넣어주게 됩니다.\n",
        "        \"\"\"\n",
        "        if training:\n",
        "            encoder_inputs, decoder_inputs = inputs\n",
        "            context_vector = self.encoder(encoder_inputs)\n",
        "            decoder_outputs, _, _ = self.decoder(decoder_inputs, context_vector)\n",
        "            return decoder_outputs\n",
        "        else:\n",
        "            context_vector = self.encoder(inputs)\n",
        "            target_seq = tf.constant([[self.start_token]], dtype=tf.float32)\n",
        "            results = tf.TensorArray(tf.int32, self.time_steps)\n",
        "\n",
        "            for i in tf.range(self.time_steps):\n",
        "                decoder_output, decoder_hidden, decoder_cell = self.decoder(target_seq,\n",
        "                                                                            context_vector)\n",
        "                decoder_output = tf.cast(tf.argmax(decoder_output, axis= -1),\n",
        "                                         dtype=tf.int32)\n",
        "                decoder_output = tf.reshape(decoder_output, shape=(1, 1))\n",
        "                results = results.write(i, decoder_output)\n",
        "\n",
        "                if decoder_output == self.end_token:\n",
        "                    break\n",
        "\n",
        "                target_seq = decoder_output\n",
        "                context_vector = [decoder_hidden, decoder_cell]\n",
        "\n",
        "            return tf.reshape(results.stack(), shape=(1, self.time_steps))"
      ],
      "metadata": {
        "id": "wQVlhWoEKmWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1-3. 챗봇 만들기**\n",
        "제작한 Seq2Seq 모델을 사용하여 간단한 훈련을 통해 챗봇을 만들어보도록 하겠습니다."
      ],
      "metadata": {
        "id": "FtieMJuHMU-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1) 데이터 로드 및 전처리**\n",
        "한국어 챗봇 제작을 위해 Korpora의 한국어 문답 데이터를 사용해보도록 하겠습니다.</br>\n",
        "Github : https://github.com/ko-nlp/Korpora"
      ],
      "metadata": {
        "id": "DnYZ-0ZsNJzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#API를 사용하는 경우 pip 오류가 발생하므로 csv 형태의 Raw데이터를 그대로 가져와 사용하겠습니다.\n",
        "corpus = pd.read_csv('https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv')\n",
        "\n",
        "# 2,000개 데이터 셋만 활용 (Google Colab 일 경우 3,000개에서는 메모리 오버되는 현상 발생)\n",
        "texts = []\n",
        "pairs = []\n",
        "for i, (text, pair) in enumerate(zip(corpus['Q'], corpus['A'])):\n",
        "    texts.append(text)\n",
        "    pairs.append(pair)\n",
        "    if i >= 2000:\n",
        "        break\n",
        "\n",
        "# 데이터 체크\n",
        "list(zip(texts, pairs))[1995:2000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qvh42KtcNiPY",
        "outputId": "6ee387d4-6222-4f6f-d4a9-7dc7274e5cbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('배 아프다', '약이 필요하면 도움을 받아보세요.'),\n",
              " ('배 터지겠네', '위를 좀 쉬게 해주세요.'),\n",
              " ('배 터지겠다.', '산책 좀 해야겠네여.'),\n",
              " ('배가 너무 고파', '뭐 좀 챙겨드세요.'),\n",
              " ('배가 넘넘 고파', '저도 밥 먹고 싶어요')]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "데이터를 확인했을 때, 마침표와 같은 기호가 있는 경우가 있고, 없는 경우가 있으므로 제거해주도록 합시다.</br>\n",
        "물음표와 공백의 경우 문장 생성의 중요한 특징이 될 수 있으므로 제거하지 않습니다."
      ],
      "metadata": {
        "id": "GB1akT-aN1Xx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#regular expression(regex)를 사용\n",
        "import re\n",
        "\n",
        "def cleaning_sent(sentence):\n",
        "    '''\n",
        "    한글 및 숫자, 물음표 및 공백을 제외하고 제거하는 함수입니다.\n",
        "\n",
        "    Input:\n",
        "        sentence : str. 정제하려는 문장을 입력으로 받습니다.\n",
        "    Return:\n",
        "        정제 완료된 문장 반환. str\n",
        "    '''\n",
        "    sent = re.sub(r'[^1-9가-힣? ]', '', sentence)\n",
        "    return sent"
      ],
      "metadata": {
        "id": "XXS-r3NJOEdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 정제 함수 테스트\n",
        "cleaning_sent('오늘은 뭐해?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OyT9R3BlOSz2",
        "outputId": "b75c7a95-ec65-40fe-ed61-865cd30bc10f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'오늘은 뭐해?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "한국어를 사용하므로 한국어 전용 형태소 분석기인 Konlpy를 사용하도록 하겠습니다."
      ],
      "metadata": {
        "id": "GwxHN6TJOZXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eh6BJxvoOgDQ",
        "outputId": "f5331e3d-21e7-4754-8e89-957e42e56ed7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.4.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[K     |████████████████████████████████| 465 kB 73.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.9.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (21.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->JPype1>=0.7.0->konlpy) (3.0.9)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Konlpy에서는 한국어 품사를 태그해주는 다양한 태거를 지원하고 있습니다.</br>\n",
        "이 중에서 이번에는 트위터를 기반으로 만들어진 Okt를 활용하도록 하겠습니다.</br>\n",
        "Konlpy 형태소 분석 및 품사 태깅 공식 문서 : https://konlpy.org/ko/v0.6.0/morph/"
      ],
      "metadata": {
        "id": "zKtbUbq0OgrL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "\n",
        "def morpheme_anlysis(sentence):\n",
        "    '''\n",
        "    문장이 들어왔을 때, 형태소 단위로 분석해주는 함수입니다.\n",
        "    Input:\n",
        "        sentence : str. 형태소 분석을 하려는 문장입니다.\n",
        "    Return:\n",
        "        형태소 단위로 뛰워쓰기 되어있는 문장 반환. str\n",
        "    '''\n",
        "    return ' '.join(okt.morphs(sentence))"
      ],
      "metadata": {
        "id": "XxjIQchIOoyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "인코더와 디코더의 입력값, 출력값에 따라 문장의 형태가 조금씩 다릅니다. </br>\n",
        "따라서 각각에 맞는 형태로 문장을 바꾸어주는 함수를 제작합니다.\n",
        "\n",
        "질문 : 인코더의 Input</br>\n",
        "대답 : 디코더의 Input, Output"
      ],
      "metadata": {
        "id": "scaTuyEcOuRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_and_morph(sentence, is_question=True):\n",
        "    '''\n",
        "    문장이 질문인지 대답인지에 맞추어 알맞은 형태로 변형해주는 함수입니다.\n",
        "\n",
        "    Input:\n",
        "        sentence : str. 변형하려는 문장입니다.\n",
        "        is_question : 문장이 질문인지, 대답인지 알려주는 값입니다. 문장이면 True, 대답이면 False를 받습니다.\n",
        "\n",
        "    Return:\n",
        "        문장 타입에 따라 알맞게 변형된 문장을 반환합니다.\n",
        "    '''\n",
        "\n",
        "    sentence = cleaning_sent(sentence)\n",
        "    sentence = morpheme_anlysis(sentence)\n",
        "\n",
        "    if is_question:\n",
        "        return sentence\n",
        "    else:\n",
        "        return ('<sos> ' + sentence, sentence + ' <eos>')\n",
        "\n",
        "def preprocessing(questions, pairs):\n",
        "    '''\n",
        "    한국어 문답 데이터를 전처리하는 함수입니다.\n",
        "    정제, 형태소 분석, 모델의 인풋,아웃풋으로 사용할 수 있는 형태로 변환하는 과정을 포함하고 있습니다.\n",
        "\n",
        "    Input:\n",
        "        questions : list. 문답 데이터 중 질문으로 이루어진 리스트를 받습니다.\n",
        "        pairs : list. 문답 데이터 중 답변으로 이루어진 리스트를 받습니다.\n",
        "\n",
        "    Return:\n",
        "        질문, 인풋 형태의 문답, 아웃풋 형태의 문답\n",
        "    '''\n",
        "    answer_in = []\n",
        "    answer_out = []\n",
        "\n",
        "    pre_questions = [clean_and_morph(question, is_question=True) for question in questions]\n",
        "\n",
        "    for pair in pairs:\n",
        "        in_pair, out_pair = clean_and_morph(pair, is_question=False)\n",
        "        answer_in.append(in_pair)\n",
        "        answer_out.append(out_pair)\n",
        "\n",
        "    return pre_questions, answer_in, answer_out\n",
        "\n",
        "questions, answer_in, answer_out = preprocessing(texts, pairs)"
      ],
      "metadata": {
        "id": "eJiD8N47O6Sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#변환 확인\n",
        "print(questions[:3])\n",
        "print(answer_in[:3])\n",
        "print(answer_out[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JP97Pih1PAbH",
        "outputId": "1a5a5073-7eb0-4097-8f52-88b5e828299b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['12시 땡', '1 지망 학교 떨어졌어', '3 박 4일 놀러 가고 싶다']\n",
            "['<sos> 하루 가 또 가네요', '<sos> 위로 해 드립니다', '<sos> 여행 은 언제나 좋죠']\n",
            "['하루 가 또 가네요 <eos>', '위로 해 드립니다 <eos>', '여행 은 언제나 좋죠 <eos>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2) 토큰화**\n",
        "전처리가 완료된 데이터를 모델에 넣을 수 있는 형태로 토큰화합니다."
      ],
      "metadata": {
        "id": "-vmyQRUAPMoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#토크나이저 학습을 위해 전체 데이터 리스트를 하나 생성\n",
        "all_sentence = questions + answer_in + answer_out\n",
        "len(all_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtTAPywnPLO-",
        "outputId": "89d33019-94a1-4a48-8e25-fe7cca1cb694"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6003"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#리스트를 사용하여 tokenizer 학습\n",
        "tokenizer = Tokenizer(filters='', lower=False, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(all_sentence)"
      ],
      "metadata": {
        "id": "X4EJ6rvvPcA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#토큰 인덱스 확인\n",
        "for word, idx in tokenizer.word_index.items():\n",
        "    print(f'{word}\\t -> \\t{idx}')\n",
        "    if idx > 10:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2LeWjGUPoZC",
        "outputId": "a1266793-dc34-43ad-8a6c-1b70c8dfb49d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<OOV>\t -> \t1\n",
            "<sos>\t -> \t2\n",
            "<eos>\t -> \t3\n",
            "이\t -> \t4\n",
            "을\t -> \t5\n",
            "거\t -> \t6\n",
            "가\t -> \t7\n",
            "예요\t -> \t8\n",
            "도\t -> \t9\n",
            "해보세요\t -> \t10\n",
            "요\t -> \t11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#질문 답변 데이터 토큰화\n",
        "question_sequence = tokenizer.texts_to_sequences(questions)\n",
        "answer_in_sequence = tokenizer.texts_to_sequences(answer_in)\n",
        "answer_out_sequence = tokenizer.texts_to_sequences(answer_out)"
      ],
      "metadata": {
        "id": "9JyVEBEfPwHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#토큰화가 잘 적용되었는지 확인\n",
        "print(questions[:3])\n",
        "print(question_sequence[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uC11gvBDPzvN",
        "outputId": "eb80bd0e-cc09-4413-e575-db1bdef85e92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['12시 땡', '1 지망 학교 떨어졌어', '3 박 4일 놀러 가고 싶다']\n",
            "[[1758, 2493], [1609, 2494, 2495, 1610], [974, 1759, 1760, 213, 197, 106]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#문장 최대 토큰 수 계산\n",
        "#max_len은 모델이 생성할 수 있는 문장의 최대 길이도 되므로 고려해서 선정해야함\n",
        "def len_cal(sentences):\n",
        "    total_len = 0\n",
        "    count = 0\n",
        "    maxlen = 0\n",
        "\n",
        "    for i in sentences:\n",
        "        if maxlen < len(i):\n",
        "            maxlen = len(i)\n",
        "        total_len += len(i)\n",
        "        count += 1\n",
        "\n",
        "    print(total_len / count)\n",
        "    print(maxlen)\n",
        "\n",
        "len_cal(question_sequence)\n",
        "len_cal(answer_in_sequence)\n",
        "len_cal(answer_out_sequence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uo1KmWVJP7Oy",
        "outputId": "06d45650-f4ad-40ec-de60-a1627d7302d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.228385807096452\n",
            "12\n",
            "5.888055972013993\n",
            "20\n",
            "5.888055972013993\n",
            "20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#패딩\n",
        "max_len = 20\n",
        "question_pad = pad_sequences(question_sequence,\n",
        "                             max_len,\n",
        "                             padding= 'post')\n",
        "answer_in_pad = pad_sequences(answer_in_sequence,\n",
        "                             max_len,\n",
        "                             padding= 'post')\n",
        "answer_out_pad = pad_sequences(answer_out_sequence,\n",
        "                             max_len,\n",
        "                             padding= 'post')"
      ],
      "metadata": {
        "id": "cz2RoSlwQAKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#패딩 적용 확인\n",
        "question_pad.shape, answer_in_pad.shape, answer_out_pad.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08BUuN_-QEQK",
        "outputId": "3f0c8ea8-a73f-4f06-8cbf-6f942beb8796"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2001, 20), (2001, 20), (2001, 20))"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "encoder & decoder"
      ],
      "metadata": {
        "id": "L2GC4r45WK5b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vyevhw0TVamN"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.layers import LSTM\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3) 모델 훈련**\n",
        "데이터와 모델이 모두 준비되었으므로, 한국어 문답 데이터로 모델을 훈련합니다."
      ],
      "metadata": {
        "id": "X7CwyMuIQjYI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "현재 Seq2Seq 모델은 출력 결과가 인덱스로 이루어져있으므로, 이를 문장으로 바꿔주는 함수를 제작합니다."
      ],
      "metadata": {
        "id": "45n9AUAiSZW9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#인덱스를 단어로 변환하는 함수\n",
        "def convert_index_to_text(indexs, end_token):\n",
        "\n",
        "    sentence = ''\n",
        "\n",
        "    for index in indexs:\n",
        "        if index == end_token:\n",
        "            break;\n",
        "        if index > 0 and tokenizer.index_word[index] is not None:\n",
        "            sentence += tokenizer.index_word[index]\n",
        "        else:\n",
        "            sentence += ''\n",
        "\n",
        "        sentence += ' '\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "joGDjXOqSG1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#문장을 입력받으면 대답을 만들어내는 함수를 제작합니다.\n",
        "def make_prediction(model, question_inputs):\n",
        "    results = model(inputs=question_inputs, training=False)\n",
        "    # 변환된 인덱스를 문장으로 변환\n",
        "    results = np.asarray(results).reshape(-1)\n",
        "    return results"
      ],
      "metadata": {
        "id": "RKM9-E4fST4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#학습 파라미터\n",
        "#하이퍼 파라미터 튜닝 시 참고하기 쉽도록 한 셀에 모아 작성하는 것이 좋습니다.\n",
        "\n",
        "BUFFER_SIZE = 1000\n",
        "BATCH_SIZE = 16\n",
        "EMBEDDING_DIM = 100\n",
        "TIME_STEPS = max_len\n",
        "START_TOKEN = tokenizer.word_index['<sos>']\n",
        "END_TOKEN = tokenizer.word_index['<eos>']\n",
        "\n",
        "UNITS = 128\n",
        "\n",
        "#padding을 포함하기위해 +1\n",
        "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
        "DATA_LENGTH = len(questions)\n",
        "SAMPLE_SIZE = 5\n",
        "NUM_EPOCHS = 20"
      ],
      "metadata": {
        "id": "x0oCHzYVSksZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 훈련을 위해 모델을 선언합니다.\n",
        "seq2seq = Seq2seq(UNITS,\n",
        "                  VOCAB_SIZE,\n",
        "                  EMBEDDING_DIM,\n",
        "                  TIME_STEPS,\n",
        "                  START_TOKEN,\n",
        "                  END_TOKEN)\n",
        "\n",
        "seq2seq.compile(optimizer='adam',\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['acc'])"
      ],
      "metadata": {
        "id": "IWUo779TSwz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#학습된 모델을 저장하기위해 체크포인트를 지정합니다.\n",
        "checkpoint_path = 'model/seq2seq.ckpt'\n",
        "checkpoint = ModelCheckpoint(filepath=checkpoint_path,\n",
        "                             save_weights_only= True,\n",
        "                             save_best_only=True,\n",
        "                             monitor='loss',\n",
        "                             verbose=1\n",
        "                             )"
      ],
      "metadata": {
        "id": "3C_Vb6e7S3_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 훈련\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f'processing epoch: {epoch * 10 + 1}...')\n",
        "    seq2seq.fit([question_pad, answer_in_pad],\n",
        "                answer_out_pad,\n",
        "                epochs=10,\n",
        "                batch_size=BATCH_SIZE,\n",
        "                callbacks=[checkpoint]\n",
        "               )\n",
        "    # 랜덤한 샘플 번호 추출\n",
        "    samples = np.random.randint(DATA_LENGTH, size=SAMPLE_SIZE)\n",
        "\n",
        "    # 예측 성능 테스트\n",
        "    for idx in samples:\n",
        "        question_inputs = question_pad[idx]\n",
        "        # 문장 예측\n",
        "        results = make_prediction(seq2seq, np.expand_dims(question_inputs, 0))\n",
        "\n",
        "        # 변환된 인덱스를 문장으로 변환\n",
        "        results = convert_index_to_text(results, END_TOKEN)\n",
        "\n",
        "        print(f'Q: {questions[idx]}')\n",
        "        print(f'A: {results}\\n')\n",
        "        print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgpvSp35SopS",
        "outputId": "d74b645f-23f5-4fd5-b65d-5d298118831c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing epoch: 1...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 3.0061 - acc: 0.7012\n",
            "Epoch 1: loss improved from inf to 3.00615, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 9s 24ms/step - loss: 3.0061 - acc: 0.7012\n",
            "Epoch 2/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 1.7624 - acc: 0.7385\n",
            "Epoch 2: loss improved from 3.00615 to 1.76297, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 2s 15ms/step - loss: 1.7630 - acc: 0.7384\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 1.6354 - acc: 0.7568\n",
            "Epoch 3: loss improved from 1.76297 to 1.63541, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 1.6354 - acc: 0.7568\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 1.5666 - acc: 0.7590\n",
            "Epoch 4: loss improved from 1.63541 to 1.56664, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 2s 17ms/step - loss: 1.5666 - acc: 0.7590\n",
            "Epoch 5/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 1.5157 - acc: 0.7602\n",
            "Epoch 5: loss improved from 1.56664 to 1.51576, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 2s 17ms/step - loss: 1.5158 - acc: 0.7601\n",
            "Epoch 6/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 1.4744 - acc: 0.7643\n",
            "Epoch 6: loss improved from 1.51576 to 1.47417, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 1.4742 - acc: 0.7643\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 1.4371 - acc: 0.7672\n",
            "Epoch 7: loss improved from 1.47417 to 1.43711, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 2s 17ms/step - loss: 1.4371 - acc: 0.7672\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 1.4039 - acc: 0.7697\n",
            "Epoch 8: loss improved from 1.43711 to 1.40393, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 2s 17ms/step - loss: 1.4039 - acc: 0.7697\n",
            "Epoch 9/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 1.3751 - acc: 0.7711\n",
            "Epoch 9: loss improved from 1.40393 to 1.37456, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 1.3746 - acc: 0.7712\n",
            "Epoch 10/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 1.3503 - acc: 0.7737\n",
            "Epoch 10: loss improved from 1.37456 to 1.34655, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 1.3466 - acc: 0.7743\n",
            "Q: 내 마음 을 좀 알 아 달라 고\n",
            "A: 저 을 잘 거 예요 \n",
            "\n",
            "\n",
            "Q: 내 가 너무 쉽게 보였나 ?\n",
            "A: 저 을 잘 거 예요 \n",
            "\n",
            "\n",
            "Q: 나도 이제 아 재인 가\n",
            "A: 저 을 더 해보세요 \n",
            "\n",
            "\n",
            "Q: 긴 머리 관리 하는 거 힘들다\n",
            "A: 저 을 잘 거 예요 \n",
            "\n",
            "\n",
            "Q: 먼지 지수 어때\n",
            "A: 저 을 더 해보세요 \n",
            "\n",
            "\n",
            "processing epoch: 11...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 1.3197 - acc: 0.7764\n",
            "Epoch 1: loss improved from 1.34655 to 1.31974, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 1.3197 - acc: 0.7764\n",
            "Epoch 2/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 1.2926 - acc: 0.7803\n",
            "Epoch 2: loss improved from 1.31974 to 1.29371, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 1.2937 - acc: 0.7802\n",
            "Epoch 3/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 1.2674 - acc: 0.7835\n",
            "Epoch 3: loss improved from 1.29371 to 1.26766, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 1.2677 - acc: 0.7834\n",
            "Epoch 4/10\n",
            "121/126 [===========================>..] - ETA: 0s - loss: 1.2363 - acc: 0.7872\n",
            "Epoch 4: loss improved from 1.26766 to 1.23944, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 1.2394 - acc: 0.7867\n",
            "Epoch 5/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 1.2117 - acc: 0.7906\n",
            "Epoch 5: loss improved from 1.23944 to 1.21036, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 1.2104 - acc: 0.7908\n",
            "Epoch 6/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 1.1792 - acc: 0.7950\n",
            "Epoch 6: loss improved from 1.21036 to 1.18139, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 1.1814 - acc: 0.7946\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 1.1511 - acc: 0.7985\n",
            "Epoch 7: loss improved from 1.18139 to 1.15106, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 1.1511 - acc: 0.7985\n",
            "Epoch 8/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 1.1192 - acc: 0.8027\n",
            "Epoch 8: loss improved from 1.15106 to 1.11928, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 1.1193 - acc: 0.8029\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 1.0891 - acc: 0.8072\n",
            "Epoch 9: loss improved from 1.11928 to 1.08912, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 1.0891 - acc: 0.8072\n",
            "Epoch 10/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 1.0590 - acc: 0.8117\n",
            "Epoch 10: loss improved from 1.08912 to 1.06018, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 1.0602 - acc: 0.8115\n",
            "Q: 말 하고 후회 하면 어떡해\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "Q: 내 가 불효자 야\n",
            "A: 저 도 요 \n",
            "\n",
            "\n",
            "Q: 마른 기침 나와\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "Q: 나 는 뭐 든 할 수 있다\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "Q: 낙엽 밟는 소리 좋다\n",
            "A: 저 도 인 거 예요 \n",
            "\n",
            "\n",
            "processing epoch: 21...\n",
            "Epoch 1/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 1.0309 - acc: 0.8151\n",
            "Epoch 1: loss improved from 1.06018 to 1.03060, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 2s 12ms/step - loss: 1.0306 - acc: 0.8152\n",
            "Epoch 2/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 1.0034 - acc: 0.8195\n",
            "Epoch 2: loss improved from 1.03060 to 1.00219, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 1.0022 - acc: 0.8196\n",
            "Epoch 3/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.9737 - acc: 0.8242\n",
            "Epoch 3: loss improved from 1.00219 to 0.97502, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.9750 - acc: 0.8241\n",
            "Epoch 4/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.9476 - acc: 0.8259\n",
            "Epoch 4: loss improved from 0.97502 to 0.94752, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.9475 - acc: 0.8259\n",
            "Epoch 5/10\n",
            "121/126 [===========================>..] - ETA: 0s - loss: 0.9233 - acc: 0.8291\n",
            "Epoch 5: loss improved from 0.94752 to 0.92190, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.9219 - acc: 0.8298\n",
            "Epoch 6/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.8951 - acc: 0.8332\n",
            "Epoch 6: loss improved from 0.92190 to 0.89756, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.8976 - acc: 0.8329\n",
            "Epoch 7/10\n",
            "121/126 [===========================>..] - ETA: 0s - loss: 0.8698 - acc: 0.8368\n",
            "Epoch 7: loss improved from 0.89756 to 0.87161, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.8716 - acc: 0.8366\n",
            "Epoch 8/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.8459 - acc: 0.8413\n",
            "Epoch 8: loss improved from 0.87161 to 0.84969, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.8497 - acc: 0.8406\n",
            "Epoch 9/10\n",
            "121/126 [===========================>..] - ETA: 0s - loss: 0.8236 - acc: 0.8458\n",
            "Epoch 9: loss improved from 0.84969 to 0.82608, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.8261 - acc: 0.8453\n",
            "Epoch 10/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.8028 - acc: 0.8485\n",
            "Epoch 10: loss improved from 0.82608 to 0.80441, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.8044 - acc: 0.8482\n",
            "Q: 누구 냐 넌 ?\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "Q: 면허 따야하나\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "Q: 결혼 하는데 돈 얼마나 들까\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "Q: 반 배정 잘 될까 ?\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "Q: 무기력증 어떻게 극복 하지 ?\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "processing epoch: 31...\n",
            "Epoch 1/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.7828 - acc: 0.8529\n",
            "Epoch 1: loss improved from 0.80441 to 0.78185, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.7819 - acc: 0.8531\n",
            "Epoch 2/10\n",
            "121/126 [===========================>..] - ETA: 0s - loss: 0.7604 - acc: 0.8565\n",
            "Epoch 2: loss improved from 0.78185 to 0.76122, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.7612 - acc: 0.8566\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.7418 - acc: 0.8604\n",
            "Epoch 3: loss improved from 0.76122 to 0.74175, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.7418 - acc: 0.8604\n",
            "Epoch 4/10\n",
            "121/126 [===========================>..] - ETA: 0s - loss: 0.7191 - acc: 0.8662\n",
            "Epoch 4: loss improved from 0.74175 to 0.72208, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.7221 - acc: 0.8656\n",
            "Epoch 5/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.7034 - acc: 0.8683\n",
            "Epoch 5: loss improved from 0.72208 to 0.70337, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.7034 - acc: 0.8684\n",
            "Epoch 6/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.6857 - acc: 0.8726\n",
            "Epoch 6: loss improved from 0.70337 to 0.68547, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.6855 - acc: 0.8728\n",
            "Epoch 7/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.6685 - acc: 0.8762\n",
            "Epoch 7: loss improved from 0.68547 to 0.66876, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.6688 - acc: 0.8762\n",
            "Epoch 8/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.6513 - acc: 0.8799\n",
            "Epoch 8: loss improved from 0.66876 to 0.65166, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.6517 - acc: 0.8798\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.6354 - acc: 0.8816\n",
            "Epoch 9: loss improved from 0.65166 to 0.63538, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.6354 - acc: 0.8816\n",
            "Epoch 10/10\n",
            "121/126 [===========================>..] - ETA: 0s - loss: 0.6218 - acc: 0.8859\n",
            "Epoch 10: loss improved from 0.63538 to 0.62082, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.6208 - acc: 0.8859\n",
            "Q: 녹차 마실래 ?\n",
            "A: 저 도 듣고 싶네요 \n",
            "\n",
            "\n",
            "Q: 내일 하루 종일 바빠\n",
            "A: 저 도 듣고 싶네요 \n",
            "\n",
            "\n",
            "Q: 물 자주 마셔야지\n",
            "A: 잘 할 수 있을 거 예요 \n",
            "\n",
            "\n",
            "Q: 발레 배워 보려고\n",
            "A: 저 도 듣고 싶네요 \n",
            "\n",
            "\n",
            "Q: 면세 에서 뭐 사지\n",
            "A: 잘 할 수 있을 거 예요 \n",
            "\n",
            "\n",
            "processing epoch: 41...\n",
            "Epoch 1/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.6078 - acc: 0.8884\n",
            "Epoch 1: loss improved from 0.62082 to 0.60768, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.6077 - acc: 0.8884\n",
            "Epoch 2/10\n",
            "121/126 [===========================>..] - ETA: 0s - loss: 0.5913 - acc: 0.8921\n",
            "Epoch 2: loss improved from 0.60768 to 0.59232, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.5923 - acc: 0.8920\n",
            "Epoch 3/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.5800 - acc: 0.8939\n",
            "Epoch 3: loss improved from 0.59232 to 0.57977, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.5798 - acc: 0.8939\n",
            "Epoch 4/10\n",
            "121/126 [===========================>..] - ETA: 0s - loss: 0.5685 - acc: 0.8968\n",
            "Epoch 4: loss improved from 0.57977 to 0.56767, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.5677 - acc: 0.8970\n",
            "Epoch 5/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.5566 - acc: 0.8987\n",
            "Epoch 5: loss improved from 0.56767 to 0.55675, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.5568 - acc: 0.8987\n",
            "Epoch 6/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.5437 - acc: 0.9014\n",
            "Epoch 6: loss improved from 0.55675 to 0.54372, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.5437 - acc: 0.9014\n",
            "Epoch 7/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.5314 - acc: 0.9036\n",
            "Epoch 7: loss improved from 0.54372 to 0.53252, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.5325 - acc: 0.9035\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.5235 - acc: 0.9054\n",
            "Epoch 8: loss improved from 0.53252 to 0.52351, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.5235 - acc: 0.9054\n",
            "Epoch 9/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.5152 - acc: 0.9071\n",
            "Epoch 9: loss improved from 0.52351 to 0.51473, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.5147 - acc: 0.9071\n",
            "Epoch 10/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.5027 - acc: 0.9098\n",
            "Epoch 10: loss improved from 0.51473 to 0.50303, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.5030 - acc: 0.9097\n",
            "Q: 미운 짓 만 해\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "Q: 나 만 야근 해\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "Q: 나 무시 당한 거 같아\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "Q: 낭만 이 사라진 것 같아\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "Q: 남자친구 한테 질린 거 같아\n",
            "A: 저 도 보고 싶어요 \n",
            "\n",
            "\n",
            "processing epoch: 51...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.4954 - acc: 0.9102\n",
            "Epoch 1: loss improved from 0.50303 to 0.49542, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.4954 - acc: 0.9102\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.4859 - acc: 0.9119\n",
            "Epoch 2: loss improved from 0.49542 to 0.48592, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.4859 - acc: 0.9119\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.4794 - acc: 0.9124\n",
            "Epoch 3: loss improved from 0.48592 to 0.47943, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.4794 - acc: 0.9124\n",
            "Epoch 4/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.4705 - acc: 0.9142\n",
            "Epoch 4: loss improved from 0.47943 to 0.47107, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.4711 - acc: 0.9140\n",
            "Epoch 5/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.4632 - acc: 0.9157\n",
            "Epoch 5: loss improved from 0.47107 to 0.46369, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.4637 - acc: 0.9154\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.4561 - acc: 0.9174\n",
            "Epoch 6: loss improved from 0.46369 to 0.45612, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 2s 12ms/step - loss: 0.4561 - acc: 0.9174\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.4511 - acc: 0.9185\n",
            "Epoch 7: loss improved from 0.45612 to 0.45112, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.4511 - acc: 0.9185\n",
            "Epoch 8/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.4437 - acc: 0.9190\n",
            "Epoch 8: loss improved from 0.45112 to 0.44403, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.4440 - acc: 0.9189\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.4391 - acc: 0.9193\n",
            "Epoch 9: loss improved from 0.44403 to 0.43909, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.4391 - acc: 0.9193\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.4326 - acc: 0.9207\n",
            "Epoch 10: loss improved from 0.43909 to 0.43262, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.4326 - acc: 0.9207\n",
            "Q: 교양 수업 은근 재미 져\n",
            "A: 저 도 듣고 싶어요 \n",
            "\n",
            "\n",
            "Q: 레시피 대로 했는데 왜 맛 이 없지 ?\n",
            "A: 저 도 듣고 싶어요 \n",
            "\n",
            "\n",
            "Q: 면접 만 보면 돼\n",
            "A: 저 도 듣고 싶어요 \n",
            "\n",
            "\n",
            "Q: 나른하다\n",
            "A: 저 도 듣고 싶어요 \n",
            "\n",
            "\n",
            "Q: 뒤 에서 얘기 하면 내 가 못 들을 줄 알았나 봐 ?\n",
            "A: 잘 할 수 있을 거 예요 \n",
            "\n",
            "\n",
            "processing epoch: 61...\n",
            "Epoch 1/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.4260 - acc: 0.9213\n",
            "Epoch 1: loss improved from 0.43262 to 0.42695, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.4270 - acc: 0.9210\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.4219 - acc: 0.9222\n",
            "Epoch 2: loss improved from 0.42695 to 0.42188, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.4219 - acc: 0.9222\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.4148 - acc: 0.9236\n",
            "Epoch 3: loss improved from 0.42188 to 0.41477, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.4148 - acc: 0.9236\n",
            "Epoch 4/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.4091 - acc: 0.9242\n",
            "Epoch 4: loss improved from 0.41477 to 0.40912, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.4091 - acc: 0.9242\n",
            "Epoch 5/10\n",
            "121/126 [===========================>..] - ETA: 0s - loss: 0.4048 - acc: 0.9257\n",
            "Epoch 5: loss improved from 0.40912 to 0.40503, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.4050 - acc: 0.9256\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.3995 - acc: 0.9261\n",
            "Epoch 6: loss improved from 0.40503 to 0.39947, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.3995 - acc: 0.9261\n",
            "Epoch 7/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.3942 - acc: 0.9266\n",
            "Epoch 7: loss improved from 0.39947 to 0.39473, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.3947 - acc: 0.9265\n",
            "Epoch 8/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.3904 - acc: 0.9277\n",
            "Epoch 8: loss improved from 0.39473 to 0.39020, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.3902 - acc: 0.9277\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.3842 - acc: 0.9288\n",
            "Epoch 9: loss improved from 0.39020 to 0.38418, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.3842 - acc: 0.9288\n",
            "Epoch 10/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.3801 - acc: 0.9291\n",
            "Epoch 10: loss improved from 0.38418 to 0.38016, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.3802 - acc: 0.9291\n",
            "Q: 무사 안일 기원\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "Q: 도와준 사람 들 너무 고맙다\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "Q: 뒤 에서 얘기 하면 내 가 못 들을 줄 알았나 봐 ?\n",
            "A: 잘 하실 거 예요 \n",
            "\n",
            "\n",
            "Q: 농담 처럼 진담 하는 사람\n",
            "A: 저 도 듣고 싶어요 \n",
            "\n",
            "\n",
            "Q: 군대 갔다 올 때 까지 기다릴 수 있을까\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "processing epoch: 71...\n",
            "Epoch 1/10\n",
            "121/126 [===========================>..] - ETA: 0s - loss: 0.3758 - acc: 0.9296\n",
            "Epoch 1: loss improved from 0.38016 to 0.37580, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.3758 - acc: 0.9296\n",
            "Epoch 2/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.3723 - acc: 0.9298\n",
            "Epoch 2: loss improved from 0.37580 to 0.37213, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.3721 - acc: 0.9299\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.3670 - acc: 0.9314\n",
            "Epoch 3: loss improved from 0.37213 to 0.36703, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.3670 - acc: 0.9314\n",
            "Epoch 4/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.3634 - acc: 0.9311\n",
            "Epoch 4: loss improved from 0.36703 to 0.36372, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.3637 - acc: 0.9312\n",
            "Epoch 5/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.3578 - acc: 0.9327\n",
            "Epoch 5: loss improved from 0.36372 to 0.35798, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 2s 13ms/step - loss: 0.3580 - acc: 0.9327\n",
            "Epoch 6/10\n",
            "121/126 [===========================>..] - ETA: 0s - loss: 0.3538 - acc: 0.9328\n",
            "Epoch 6: loss improved from 0.35798 to 0.35438, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.3544 - acc: 0.9328\n",
            "Epoch 7/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.3517 - acc: 0.9331\n",
            "Epoch 7: loss improved from 0.35438 to 0.35203, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.3520 - acc: 0.9330\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.3488 - acc: 0.9336\n",
            "Epoch 8: loss improved from 0.35203 to 0.34880, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 2s 12ms/step - loss: 0.3488 - acc: 0.9336\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.3438 - acc: 0.9337\n",
            "Epoch 9: loss improved from 0.34880 to 0.34377, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.3438 - acc: 0.9337\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.3417 - acc: 0.9343\n",
            "Epoch 10: loss improved from 0.34377 to 0.34167, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.3417 - acc: 0.9343\n",
            "Q: 내 마음 을 알아줬으면\n",
            "A: 맛있게 드세요 \n",
            "\n",
            "\n",
            "Q: 반 배정 내 가 원하는 대로 될까\n",
            "A: 잘 되길 바랍니다 \n",
            "\n",
            "\n",
            "Q: 면도기 새로 사야 돼\n",
            "A: 저 도 요 \n",
            "\n",
            "\n",
            "Q: 넘 넘 외로워 죽겠어\n",
            "A: 저 도 보고 싶어요 \n",
            "\n",
            "\n",
            "Q: 계속 엇갈리는 느낌\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "processing epoch: 81...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.3362 - acc: 0.9355\n",
            "Epoch 1: loss improved from 0.34167 to 0.33619, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.3362 - acc: 0.9355\n",
            "Epoch 2/10\n",
            "121/126 [===========================>..] - ETA: 0s - loss: 0.3322 - acc: 0.9359\n",
            "Epoch 2: loss improved from 0.33619 to 0.33222, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.3322 - acc: 0.9359\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.3295 - acc: 0.9365\n",
            "Epoch 3: loss improved from 0.33222 to 0.32955, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.3295 - acc: 0.9365\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.3252 - acc: 0.9363\n",
            "Epoch 4: loss improved from 0.32955 to 0.32520, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.3252 - acc: 0.9363\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.3264 - acc: 0.9362\n",
            "Epoch 5: loss did not improve from 0.32520\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.3264 - acc: 0.9362\n",
            "Epoch 6/10\n",
            "121/126 [===========================>..] - ETA: 0s - loss: 0.3173 - acc: 0.9382\n",
            "Epoch 6: loss improved from 0.32520 to 0.31742, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.3174 - acc: 0.9384\n",
            "Epoch 7/10\n",
            "121/126 [===========================>..] - ETA: 0s - loss: 0.3136 - acc: 0.9388\n",
            "Epoch 7: loss improved from 0.31742 to 0.31387, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.3139 - acc: 0.9388\n",
            "Epoch 8/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.3093 - acc: 0.9387\n",
            "Epoch 8: loss improved from 0.31387 to 0.30930, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.3093 - acc: 0.9387\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.3059 - acc: 0.9396\n",
            "Epoch 9: loss improved from 0.30930 to 0.30586, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.3059 - acc: 0.9396\n",
            "Epoch 10/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.3018 - acc: 0.9393\n",
            "Epoch 10: loss improved from 0.30586 to 0.30182, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.3018 - acc: 0.9393\n",
            "Q: 더워 미치겠다\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "Q: 믿는 게 아니었어\n",
            "A: 잘 하실 거 예요 \n",
            "\n",
            "\n",
            "Q: 결혼 하는데 돈 얼마나 들까\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "Q: 방 에 먼지 가 한가득\n",
            "A: 좋은 사람과 함께 가세 요 \n",
            "\n",
            "\n",
            "Q: 나 만 설레는 거야\n",
            "A: 좋은 생각 이에요 \n",
            "\n",
            "\n",
            "processing epoch: 91...\n",
            "Epoch 1/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.2964 - acc: 0.9412\n",
            "Epoch 1: loss improved from 0.30182 to 0.29642, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.2964 - acc: 0.9412\n",
            "Epoch 2/10\n",
            "121/126 [===========================>..] - ETA: 0s - loss: 0.2930 - acc: 0.9411\n",
            "Epoch 2: loss improved from 0.29642 to 0.29380, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.2938 - acc: 0.9410\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.2892 - acc: 0.9419\n",
            "Epoch 3: loss improved from 0.29380 to 0.28922, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.2892 - acc: 0.9419\n",
            "Epoch 4/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.2850 - acc: 0.9426\n",
            "Epoch 4: loss improved from 0.28922 to 0.28503, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.2850 - acc: 0.9426\n",
            "Epoch 5/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.2818 - acc: 0.9425\n",
            "Epoch 5: loss improved from 0.28503 to 0.28204, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.2820 - acc: 0.9424\n",
            "Epoch 6/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.2780 - acc: 0.9432\n",
            "Epoch 6: loss improved from 0.28204 to 0.27792, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.2779 - acc: 0.9432\n",
            "Epoch 7/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.2744 - acc: 0.9438\n",
            "Epoch 7: loss improved from 0.27792 to 0.27440, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.2744 - acc: 0.9438\n",
            "Epoch 8/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.2728 - acc: 0.9435\n",
            "Epoch 8: loss improved from 0.27440 to 0.27273, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.2727 - acc: 0.9435\n",
            "Epoch 9/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.2688 - acc: 0.9444\n",
            "Epoch 9: loss improved from 0.27273 to 0.26910, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.2691 - acc: 0.9444\n",
            "Epoch 10/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.2664 - acc: 0.9441\n",
            "Epoch 10: loss improved from 0.26910 to 0.26648, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 2s 13ms/step - loss: 0.2665 - acc: 0.9440\n",
            "Q: 나도 비키니 입고 싶다\n",
            "A: 눈 이 많이 내렸나 봐요 \n",
            "\n",
            "\n",
            "Q: 기분 이 그 지 같아\n",
            "A: 잠시 눈 을 붙 여보세요 \n",
            "\n",
            "\n",
            "Q: 다른 사람 들 에게 사랑 을 베풀고 싶어\n",
            "A: 자신 의 독특함을 믿으세요 \n",
            "\n",
            "\n",
            "Q: 도시락 싸서 피크닉 가고 싶어\n",
            "A: 마음 이 아픈가요 \n",
            "\n",
            "\n",
            "Q: 담배 너무 비쌈\n",
            "A: 잠시 눈 을 붙 여보세요 \n",
            "\n",
            "\n",
            "processing epoch: 101...\n",
            "Epoch 1/10\n",
            "121/126 [===========================>..] - ETA: 0s - loss: 0.2639 - acc: 0.9446\n",
            "Epoch 1: loss improved from 0.26648 to 0.26382, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.2638 - acc: 0.9446\n",
            "Epoch 2/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.2621 - acc: 0.9448\n",
            "Epoch 2: loss improved from 0.26382 to 0.26210, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.2621 - acc: 0.9448\n",
            "Epoch 3/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.2576 - acc: 0.9458\n",
            "Epoch 3: loss improved from 0.26210 to 0.25803, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.2580 - acc: 0.9457\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.2553 - acc: 0.9461\n",
            "Epoch 4: loss improved from 0.25803 to 0.25527, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.2553 - acc: 0.9461\n",
            "Epoch 5/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.2518 - acc: 0.9470\n",
            "Epoch 5: loss improved from 0.25527 to 0.25180, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.2518 - acc: 0.9470\n",
            "Epoch 6/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.2523 - acc: 0.9468\n",
            "Epoch 6: loss did not improve from 0.25180\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.2521 - acc: 0.9468\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.2481 - acc: 0.9468\n",
            "Epoch 7: loss improved from 0.25180 to 0.24809, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.2481 - acc: 0.9468\n",
            "Epoch 8/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.2448 - acc: 0.9478\n",
            "Epoch 8: loss improved from 0.24809 to 0.24480, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.2448 - acc: 0.9478\n",
            "Epoch 9/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.2434 - acc: 0.9478\n",
            "Epoch 9: loss improved from 0.24480 to 0.24339, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.2434 - acc: 0.9479\n",
            "Epoch 10/10\n",
            "121/126 [===========================>..] - ETA: 0s - loss: 0.2395 - acc: 0.9480\n",
            "Epoch 10: loss improved from 0.24339 to 0.23912, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.2391 - acc: 0.9481\n",
            "Q: 나 만 친구 라고 생각 한 건가\n",
            "A: 스트레스 를 받으시나 봐요 \n",
            "\n",
            "\n",
            "Q: 꼴 사나워질 것 같은데\n",
            "A: 눈 을 깜빡 거려 보세요 \n",
            "\n",
            "\n",
            "Q: 떡볶이 먹고 싶다\n",
            "A: 드세요 \n",
            "\n",
            "\n",
            "Q: 고민 좀 들어줄래\n",
            "A: 지금 도 충분해요 \n",
            "\n",
            "\n",
            "Q: 과거 는 잊고 앞 으로 나아 가야 지\n",
            "A: 오늘이 중요하죠 \n",
            "\n",
            "\n",
            "processing epoch: 111...\n",
            "Epoch 1/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.2352 - acc: 0.9493\n",
            "Epoch 1: loss improved from 0.23912 to 0.23559, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.2356 - acc: 0.9493\n",
            "Epoch 2/10\n",
            "121/126 [===========================>..] - ETA: 0s - loss: 0.2319 - acc: 0.9494\n",
            "Epoch 2: loss improved from 0.23559 to 0.23235, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.2323 - acc: 0.9493\n",
            "Epoch 3/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.2308 - acc: 0.9494\n",
            "Epoch 3: loss improved from 0.23235 to 0.23074, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.2307 - acc: 0.9494\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.2277 - acc: 0.9504\n",
            "Epoch 4: loss improved from 0.23074 to 0.22771, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.2277 - acc: 0.9504\n",
            "Epoch 5/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.2241 - acc: 0.9501\n",
            "Epoch 5: loss improved from 0.22771 to 0.22407, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.2241 - acc: 0.9502\n",
            "Epoch 6/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.2211 - acc: 0.9513\n",
            "Epoch 6: loss improved from 0.22407 to 0.22099, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.2210 - acc: 0.9512\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.2196 - acc: 0.9509\n",
            "Epoch 7: loss improved from 0.22099 to 0.21963, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.2196 - acc: 0.9509\n",
            "Epoch 8/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.2145 - acc: 0.9520\n",
            "Epoch 8: loss improved from 0.21963 to 0.21522, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.2152 - acc: 0.9519\n",
            "Epoch 9/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.2143 - acc: 0.9516\n",
            "Epoch 9: loss improved from 0.21522 to 0.21407, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.2141 - acc: 0.9517\n",
            "Epoch 10/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.2091 - acc: 0.9529\n",
            "Epoch 10: loss improved from 0.21407 to 0.20935, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 2s 13ms/step - loss: 0.2093 - acc: 0.9528\n",
            "Q: 돈 다 어디 쓴지 모르겠네\n",
            "A: 눈 체조 를 해보세요 \n",
            "\n",
            "\n",
            "Q: 물 마셔야지\n",
            "A: 좋은 건강 습관 이에요 \n",
            "\n",
            "\n",
            "Q: 밥 먹어야지\n",
            "A: 저 를 만들어 준 사람 을 부모님 저 랑 이야기 해 주는 사람 을 친구 로 생각 하고 있어요 \n",
            "\n",
            "\n",
            "Q: 맞춤법 을 자꾸 틀리는 남친 깬다\n",
            "A: 당신 을 인정 해줄 곳 을 찾아보세요 \n",
            "\n",
            "\n",
            "Q: 기댈 수 있는 사람\n",
            "A: 세상 에는 별의별 사람 들 이 있어요 \n",
            "\n",
            "\n",
            "processing epoch: 121...\n",
            "Epoch 1/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.2055 - acc: 0.9525\n",
            "Epoch 1: loss improved from 0.20935 to 0.20557, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.2056 - acc: 0.9525\n",
            "Epoch 2/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.2003 - acc: 0.9537\n",
            "Epoch 2: loss improved from 0.20557 to 0.20026, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.2003 - acc: 0.9537\n",
            "Epoch 3/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.1976 - acc: 0.9548\n",
            "Epoch 3: loss improved from 0.20026 to 0.19760, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.1976 - acc: 0.9548\n",
            "Epoch 4/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.1954 - acc: 0.9550\n",
            "Epoch 4: loss improved from 0.19760 to 0.19546, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.1955 - acc: 0.9550\n",
            "Epoch 5/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.1914 - acc: 0.9552\n",
            "Epoch 5: loss improved from 0.19546 to 0.19159, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.1916 - acc: 0.9552\n",
            "Epoch 6/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.1898 - acc: 0.9558\n",
            "Epoch 6: loss improved from 0.19159 to 0.18959, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.1896 - acc: 0.9559\n",
            "Epoch 7/10\n",
            "121/126 [===========================>..] - ETA: 0s - loss: 0.1856 - acc: 0.9566\n",
            "Epoch 7: loss improved from 0.18959 to 0.18630, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.1863 - acc: 0.9564\n",
            "Epoch 8/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.1836 - acc: 0.9562\n",
            "Epoch 8: loss improved from 0.18630 to 0.18355, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.1836 - acc: 0.9562\n",
            "Epoch 9/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.1806 - acc: 0.9569\n",
            "Epoch 9: loss improved from 0.18355 to 0.18056, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.1806 - acc: 0.9569\n",
            "Epoch 10/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.1795 - acc: 0.9574\n",
            "Epoch 10: loss improved from 0.18056 to 0.17910, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.1791 - acc: 0.9576\n",
            "Q: 강원도 가서 살까 ?\n",
            "A: 아름다운 곳 이 죠 \n",
            "\n",
            "\n",
            "Q: 고민 있어\n",
            "A: 지금 도 충분해요 \n",
            "\n",
            "\n",
            "Q: 금수 저 로 태어났으면\n",
            "A: 아이 를 금수 저 로 만들어주세요 \n",
            "\n",
            "\n",
            "Q: 남 들 이 다 손가락질 하는 거 같아\n",
            "A: 남 들 눈 은 신경 쓰지 마세요 \n",
            "\n",
            "\n",
            "Q: 너 도 고민 있니\n",
            "A: 지금 도 충분해요 \n",
            "\n",
            "\n",
            "processing epoch: 131...\n",
            "Epoch 1/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.1732 - acc: 0.9591\n",
            "Epoch 1: loss improved from 0.17910 to 0.17311, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.1731 - acc: 0.9591\n",
            "Epoch 2/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.1682 - acc: 0.9600\n",
            "Epoch 2: loss improved from 0.17311 to 0.16832, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.1683 - acc: 0.9599\n",
            "Epoch 3/10\n",
            "121/126 [===========================>..] - ETA: 0s - loss: 0.1670 - acc: 0.9593\n",
            "Epoch 3: loss improved from 0.16832 to 0.16729, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.1673 - acc: 0.9592\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.1644 - acc: 0.9599\n",
            "Epoch 4: loss improved from 0.16729 to 0.16441, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.1644 - acc: 0.9599\n",
            "Epoch 5/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.1596 - acc: 0.9613\n",
            "Epoch 5: loss improved from 0.16441 to 0.15985, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.1599 - acc: 0.9612\n",
            "Epoch 6/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.1563 - acc: 0.9620\n",
            "Epoch 6: loss improved from 0.15985 to 0.15625, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.1563 - acc: 0.9620\n",
            "Epoch 7/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.1533 - acc: 0.9624\n",
            "Epoch 7: loss improved from 0.15625 to 0.15330, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 2s 12ms/step - loss: 0.1533 - acc: 0.9623\n",
            "Epoch 8/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.1518 - acc: 0.9621\n",
            "Epoch 8: loss improved from 0.15330 to 0.15225, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.1523 - acc: 0.9620\n",
            "Epoch 9/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.1471 - acc: 0.9637\n",
            "Epoch 9: loss improved from 0.15225 to 0.14741, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.1474 - acc: 0.9635\n",
            "Epoch 10/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.1497 - acc: 0.9630\n",
            "Epoch 10: loss did not improve from 0.14741\n",
            "126/126 [==============================] - 2s 13ms/step - loss: 0.1498 - acc: 0.9629\n",
            "Q: 꿈 이 두 개야\n",
            "A: 돈 은 다시 들어올 거 예요 \n",
            "\n",
            "\n",
            "Q: 보면 나 만 빼고 다 행복 해보여\n",
            "A: 회사 와 자신 에 대해 서 더 공부 해서 자신감 을 가져 보세요 \n",
            "\n",
            "\n",
            "Q: 맞는 결정 을 한거겠지 ?\n",
            "A: 네 이제 잘 해낼 차례 예요 \n",
            "\n",
            "\n",
            "Q: 시간 낭비 인 거 아는데 매일 하는 중\n",
            "A: 시간 을 정 하고 해보세요 \n",
            "\n",
            "\n",
            "Q: 냉장고 에 먹을 게 없네\n",
            "A: 장 보러 가봅시다 \n",
            "\n",
            "\n",
            "processing epoch: 141...\n",
            "Epoch 1/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.1415 - acc: 0.9644\n",
            "Epoch 1: loss improved from 0.14741 to 0.14202, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.1420 - acc: 0.9642\n",
            "Epoch 2/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.1386 - acc: 0.9657\n",
            "Epoch 2: loss improved from 0.14202 to 0.13862, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.1386 - acc: 0.9657\n",
            "Epoch 3/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.1408 - acc: 0.9644\n",
            "Epoch 3: loss did not improve from 0.13862\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.1406 - acc: 0.9644\n",
            "Epoch 4/10\n",
            "121/126 [===========================>..] - ETA: 0s - loss: 0.1332 - acc: 0.9666\n",
            "Epoch 4: loss improved from 0.13862 to 0.13388, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.1339 - acc: 0.9664\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.1321 - acc: 0.9671\n",
            "Epoch 5: loss improved from 0.13388 to 0.13213, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.1321 - acc: 0.9671\n",
            "Epoch 6/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.1295 - acc: 0.9678\n",
            "Epoch 6: loss improved from 0.13213 to 0.12952, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.1295 - acc: 0.9678\n",
            "Epoch 7/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.1257 - acc: 0.9674\n",
            "Epoch 7: loss improved from 0.12952 to 0.12578, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.1258 - acc: 0.9674\n",
            "Epoch 8/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.1227 - acc: 0.9691\n",
            "Epoch 8: loss improved from 0.12578 to 0.12274, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.1227 - acc: 0.9691\n",
            "Epoch 9/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.1219 - acc: 0.9690\n",
            "Epoch 9: loss improved from 0.12274 to 0.12206, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.1221 - acc: 0.9690\n",
            "Epoch 10/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.1230 - acc: 0.9694\n",
            "Epoch 10: loss did not improve from 0.12206\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.1226 - acc: 0.9695\n",
            "Q: 내 문제점 이 뭘 까\n",
            "A: 오늘 은 쉬세요 \n",
            "\n",
            "\n",
            "Q: 고구마 다이어트 해야지\n",
            "A: 너무 미워하지 마세요 \n",
            "\n",
            "\n",
            "Q: 그냥 잘 못 했다고 하면 될거 같은데 자꾸 변명 해\n",
            "A: 피 할 수 있으면 피 하세요 \n",
            "\n",
            "\n",
            "Q: 발목 다쳤어\n",
            "A: 병원 가보세요 \n",
            "\n",
            "\n",
            "Q: 고백 하고 후회 하면 어떡하지\n",
            "A: 후회 는 후회 를 낳을 뿐 이에요 용기 내세 요 \n",
            "\n",
            "\n",
            "processing epoch: 151...\n",
            "Epoch 1/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.1144 - acc: 0.9713\n",
            "Epoch 1: loss improved from 0.12206 to 0.11433, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.1143 - acc: 0.9713\n",
            "Epoch 2/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.1119 - acc: 0.9719\n",
            "Epoch 2: loss improved from 0.11433 to 0.11181, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.1118 - acc: 0.9719\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.1069 - acc: 0.9727\n",
            "Epoch 3: loss improved from 0.11181 to 0.10689, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.1069 - acc: 0.9727\n",
            "Epoch 4/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.1067 - acc: 0.9731\n",
            "Epoch 4: loss improved from 0.10689 to 0.10672, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.1067 - acc: 0.9731\n",
            "Epoch 5/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.1033 - acc: 0.9739\n",
            "Epoch 5: loss improved from 0.10672 to 0.10324, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.1032 - acc: 0.9739\n",
            "Epoch 6/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.1008 - acc: 0.9740\n",
            "Epoch 6: loss improved from 0.10324 to 0.10086, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.1009 - acc: 0.9740\n",
            "Epoch 7/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0987 - acc: 0.9754\n",
            "Epoch 7: loss improved from 0.10086 to 0.09863, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0986 - acc: 0.9755\n",
            "Epoch 8/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0974 - acc: 0.9757\n",
            "Epoch 8: loss improved from 0.09863 to 0.09733, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0973 - acc: 0.9757\n",
            "Epoch 9/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.0950 - acc: 0.9768\n",
            "Epoch 9: loss improved from 0.09733 to 0.09498, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0950 - acc: 0.9768\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0920 - acc: 0.9773\n",
            "Epoch 10: loss improved from 0.09498 to 0.09200, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0920 - acc: 0.9773\n",
            "Q: 말 잘 하고 싶어\n",
            "A: 나 만의 공부 방법 을 찾아보세요 \n",
            "\n",
            "\n",
            "Q: 몸 이 무거워\n",
            "A: 피로 를 풀어야 할 것 같아요 \n",
            "\n",
            "\n",
            "Q: 꽃다발 말리 면 에쁘겠 지\n",
            "A: 거꾸로 해서 드라이 플라워 만들어 보세요 \n",
            "\n",
            "\n",
            "Q: 고데기 망했어\n",
            "A: 연습 이 필요해요 \n",
            "\n",
            "\n",
            "Q: 다 알 고 있을까 ?\n",
            "A: 다 알 수 는 없어요 \n",
            "\n",
            "\n",
            "processing epoch: 161...\n",
            "Epoch 1/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0903 - acc: 0.9778\n",
            "Epoch 1: loss improved from 0.09200 to 0.09034, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 2s 17ms/step - loss: 0.0903 - acc: 0.9778\n",
            "Epoch 2/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0900 - acc: 0.9774\n",
            "Epoch 2: loss did not improve from 0.09034\n",
            "126/126 [==============================] - 1s 12ms/step - loss: 0.0904 - acc: 0.9773\n",
            "Epoch 3/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0904 - acc: 0.9780\n",
            "Epoch 3: loss did not improve from 0.09034\n",
            "126/126 [==============================] - 2s 13ms/step - loss: 0.0905 - acc: 0.9780\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0900 - acc: 0.9771\n",
            "Epoch 4: loss improved from 0.09034 to 0.08997, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0900 - acc: 0.9771\n",
            "Epoch 5/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.0822 - acc: 0.9798\n",
            "Epoch 5: loss improved from 0.08997 to 0.08180, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0818 - acc: 0.9799\n",
            "Epoch 6/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0804 - acc: 0.9805\n",
            "Epoch 6: loss improved from 0.08180 to 0.08038, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0804 - acc: 0.9805\n",
            "Epoch 7/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0797 - acc: 0.9812\n",
            "Epoch 7: loss improved from 0.08038 to 0.08001, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0800 - acc: 0.9811\n",
            "Epoch 8/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0744 - acc: 0.9822\n",
            "Epoch 8: loss improved from 0.08001 to 0.07442, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0744 - acc: 0.9822\n",
            "Epoch 9/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0806 - acc: 0.9800\n",
            "Epoch 9: loss did not improve from 0.07442\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0806 - acc: 0.9800\n",
            "Epoch 10/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0721 - acc: 0.9822\n",
            "Epoch 10: loss improved from 0.07442 to 0.07212, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0721 - acc: 0.9822\n",
            "Q: 뜻밖 의 고민 을 하고 있어\n",
            "A: 인생 은 항상 마음 먹은 대로 되지 않아요 \n",
            "\n",
            "\n",
            "Q: 면접 이 코앞 이야\n",
            "A: 잘 볼 수 있을 거 예요 \n",
            "\n",
            "\n",
            "Q: 낌새 가 이상하더니 딱 걸렸어\n",
            "A: 잘 해결 되길 바라요 \n",
            "\n",
            "\n",
            "Q: 너 누구 니 ?\n",
            "A: 저 는 위로 해드리는 로봇 이에요 \n",
            "\n",
            "\n",
            "Q: 목표 가 없어\n",
            "A: 목표 는 있으면 좋겠지만 없다고 미리 걱정 할 필요 없어요 생길 거 예요 \n",
            "\n",
            "\n",
            "processing epoch: 171...\n",
            "Epoch 1/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0694 - acc: 0.9833\n",
            "Epoch 1: loss improved from 0.07212 to 0.06939, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0694 - acc: 0.9833\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0675 - acc: 0.9837\n",
            "Epoch 2: loss improved from 0.06939 to 0.06753, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0675 - acc: 0.9837\n",
            "Epoch 3/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0653 - acc: 0.9844\n",
            "Epoch 3: loss improved from 0.06753 to 0.06542, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0654 - acc: 0.9843\n",
            "Epoch 4/10\n",
            "121/126 [===========================>..] - ETA: 0s - loss: 0.0656 - acc: 0.9848\n",
            "Epoch 4: loss did not improve from 0.06542\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0656 - acc: 0.9848\n",
            "Epoch 5/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0628 - acc: 0.9846\n",
            "Epoch 5: loss improved from 0.06542 to 0.06264, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0626 - acc: 0.9846\n",
            "Epoch 6/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0591 - acc: 0.9865\n",
            "Epoch 6: loss improved from 0.06264 to 0.05918, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0592 - acc: 0.9865\n",
            "Epoch 7/10\n",
            "121/126 [===========================>..] - ETA: 0s - loss: 0.0626 - acc: 0.9850\n",
            "Epoch 7: loss did not improve from 0.05918\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0621 - acc: 0.9851\n",
            "Epoch 8/10\n",
            "121/126 [===========================>..] - ETA: 0s - loss: 0.0572 - acc: 0.9873\n",
            "Epoch 8: loss improved from 0.05918 to 0.05714, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0571 - acc: 0.9874\n",
            "Epoch 9/10\n",
            "121/126 [===========================>..] - ETA: 0s - loss: 0.0563 - acc: 0.9871\n",
            "Epoch 9: loss improved from 0.05714 to 0.05654, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0565 - acc: 0.9869\n",
            "Epoch 10/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.0619 - acc: 0.9854\n",
            "Epoch 10: loss did not improve from 0.05654\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0618 - acc: 0.9855\n",
            "Q: 또 개강 이야\n",
            "A: 즐거운 시작 되길 바랍니다 \n",
            "\n",
            "\n",
            "Q: 단거 땡긴다\n",
            "A: 맛있게 먹으면 칼로리 \n",
            "\n",
            "\n",
            "Q: 눈 이 안 떠져\n",
            "A: 잠시 눈 을 붙 여보세요 \n",
            "\n",
            "\n",
            "Q: 담배 끊었었는데 담배 피고 싶다\n",
            "A: 그 때 가 고비 예요 한번 참아 보세요 \n",
            "\n",
            "\n",
            "Q: 너 말 제대로 못 해 ?\n",
            "A: 제 가 아직 많이 부족합니다 \n",
            "\n",
            "\n",
            "processing epoch: 181...\n",
            "Epoch 1/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.9875\n",
            "Epoch 1: loss improved from 0.05654 to 0.05559, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0556 - acc: 0.9875\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0531 - acc: 0.9880\n",
            "Epoch 2: loss improved from 0.05559 to 0.05309, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0531 - acc: 0.9880\n",
            "Epoch 3/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9885\n",
            "Epoch 3: loss improved from 0.05309 to 0.05159, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0516 - acc: 0.9885\n",
            "Epoch 4/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.0504 - acc: 0.9889\n",
            "Epoch 4: loss improved from 0.05159 to 0.05047, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0505 - acc: 0.9889\n",
            "Epoch 5/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.0488 - acc: 0.9893\n",
            "Epoch 5: loss improved from 0.05047 to 0.04844, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0484 - acc: 0.9895\n",
            "Epoch 6/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9898\n",
            "Epoch 6: loss improved from 0.04844 to 0.04752, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0475 - acc: 0.9897\n",
            "Epoch 7/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9890\n",
            "Epoch 7: loss did not improve from 0.04752\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0476 - acc: 0.9890\n",
            "Epoch 8/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9897\n",
            "Epoch 8: loss improved from 0.04752 to 0.04666, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 2s 13ms/step - loss: 0.0467 - acc: 0.9897\n",
            "Epoch 9/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9901\n",
            "Epoch 9: loss improved from 0.04666 to 0.04453, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 2s 13ms/step - loss: 0.0445 - acc: 0.9901\n",
            "Epoch 10/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9913\n",
            "Epoch 10: loss improved from 0.04453 to 0.04240, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0424 - acc: 0.9913\n",
            "Q: 결혼 하면 좋을까\n",
            "A: 서로 노력 하면 행복할 거 예요 \n",
            "\n",
            "\n",
            "Q: 많이 자도 피곤해\n",
            "A: 요즘 바쁜 가봐요 \n",
            "\n",
            "\n",
            "Q: 내 문제 는 뭘 까\n",
            "A: 오늘 은 약간 의 변화 를 줘 보세요 \n",
            "\n",
            "\n",
            "Q: 겁난다\n",
            "A: 용기 내 보세요 \n",
            "\n",
            "\n",
            "Q: 바지 입을까 ? 치마 입을까 ?\n",
            "A: 오늘 은 바지 가 좋을거 같아요 \n",
            "\n",
            "\n",
            "processing epoch: 191...\n",
            "Epoch 1/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9901\n",
            "Epoch 1: loss improved from 0.04240 to 0.04211, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0421 - acc: 0.9903\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0391 - acc: 0.9918\n",
            "Epoch 2: loss improved from 0.04211 to 0.03907, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0391 - acc: 0.9918\n",
            "Epoch 3/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9907\n",
            "Epoch 3: loss did not improve from 0.03907\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0403 - acc: 0.9907\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0406 - acc: 0.9907\n",
            "Epoch 4: loss did not improve from 0.03907\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0406 - acc: 0.9907\n",
            "Epoch 5/10\n",
            "121/126 [===========================>..] - ETA: 0s - loss: 0.0391 - acc: 0.9917\n",
            "Epoch 5: loss improved from 0.03907 to 0.03891, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0389 - acc: 0.9917\n",
            "Epoch 6/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9920\n",
            "Epoch 6: loss improved from 0.03891 to 0.03665, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0367 - acc: 0.9919\n",
            "Epoch 7/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9926\n",
            "Epoch 7: loss improved from 0.03665 to 0.03475, saving model to model/seq2seq.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0347 - acc: 0.9926\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0359 - acc: 0.9927\n",
            "Epoch 8: loss did not improve from 0.03475\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0359 - acc: 0.9927\n",
            "Epoch 9/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9923\n",
            "Epoch 9: loss did not improve from 0.03475\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0372 - acc: 0.9923\n",
            "Epoch 10/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9924\n",
            "Epoch 10: loss did not improve from 0.03475\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0359 - acc: 0.9925\n",
            "Q: 나들이 를 가볼까\n",
            "A: 같이 가요 \n",
            "\n",
            "\n",
            "Q: 배 터지겠다\n",
            "A: 산책 좀 해야 겠네여 \n",
            "\n",
            "\n",
            "Q: 못 하는 게 없는 사람\n",
            "A: 그런 사람 이 있으면 저 좀 소개 시켜주세요 \n",
            "\n",
            "\n",
            "Q: 갑자기 눈물 나\n",
            "A: 마음 이 아픈가요 \n",
            "\n",
            "\n",
            "Q: 나 는 모자란 사람인 거 같아\n",
            "A: 모자라지 않아요 \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4) 챗봇 구현 및 성능 확인**\n",
        "훈련된 Seq2Seq 모델을 가지고 간단한 대화를 할 수 있는 코드를 만들어보도록하겠습니다.</br>\n",
        "만들어진 챗봇에 다양한 질문을 넣어보면서 모델이 어떤 문장을 생성하는지 확인해봅시다."
      ],
      "metadata": {
        "id": "P9BDPzEqTiKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#챗봇에서 입력한 문장을 모델에 넣을 수 있는 형태로 변환하는 함수\n",
        "def question_to_input(sentence):\n",
        "    sentence = clean_and_morph(sentence)\n",
        "    question_sequence = tokenizer.texts_to_sequences([sentence])\n",
        "    question_pad = pad_sequences(question_sequence, maxlen= max_len, truncating='post', padding='post')\n",
        "    return question_pad\n",
        "\n",
        "#간단한 챗봇 구현 -> q를 누르면 종료됩니다.\n",
        "def run_chatbot(question):\n",
        "    question_inputs = question_to_input(question)\n",
        "    results = make_prediction(seq2seq, question_inputs)\n",
        "    results = convert_index_to_text(results, END_TOKEN)\n",
        "    return results\n",
        "\n",
        "while True:\n",
        "    user_input = input('대화를 입력하세요\\n')\n",
        "    if user_input =='q':\n",
        "        break\n",
        "    print('answer: {}'.format(run_chatbot(user_input)))"
      ],
      "metadata": {
        "id": "PE0xPYYb7PFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Attention**\n",
        "- 어텐션 메커니즘은 자연어처리 분야에서 대세 모듈로 사용되고 있는 트랜스포머의 기반이 되는 메커니즘입니다.</br>\n",
        "- 트랜스포머를 알아보기에 앞서, 어텐션 메커니즘의 이론에 대해 알아보고 Seq2Seq 모델에 어텐션을 적용해보도록 하겠습니다."
      ],
      "metadata": {
        "id": "l9jYsZF2WPmO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1-1. Seq2Seq의 한계점**\n",
        "RNN에 기반한 Seq2Seq 모델의 인코더, 디코더 구조는 문장 생성 분야에 큰 발전을 가져다 주었지만, 다양한 문제점을 가지고 있었습니다.</br>\n",
        "1. 하나의 컨텍스트 벡터에 입력 데이터의 **모든 정보를 압축**해서 디코더에 전달하므로 정보의 손실이 발생하게 됩니다.</br>\n",
        "2. RNN을 기반으로하기 때문에 기울기 소실 문제가 발생합니다.</br>\n",
        "3. RNN을 기반으로하기 때문에 순차적으로 데이터를 처리하므로 병렬처리가 불가능합니다.\n",
        "\n",
        "Seq2Seq 모델이 등장했던 시기에는 하드웨어의 성능 문제 때문에 하나의 컨텍스트 벡터에 모든 정보를 압축해야했지만, 시대가 지나면서 하드웨어가 발전함에 따라 인코더의 모든 출력 정보를 하나의 컨텍스트에 담지 않아도 충분히 전달할 수 있게 되었습니다."
      ],
      "metadata": {
        "id": "uCNQJBHwW0nh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1-2. Seq2Seq with Attention**\n",
        "어텐션 메커니즘을 적용한 시퀀스-투-시퀀스 모델은 디코더가 인코더의 모든 출력을 참고합니다.\n",
        "<img src=\"https://user-images.githubusercontent.com/45377884/86040873-b942d800-ba7f-11ea-9f59-ee23923f777e.gif\" alt=\"seq2seq_7\" width=\"800\" /></br>\n",
        "그렇다면 디코더에서 각 인코더의 출력값을 어떻게 참고하게 될까요? 더 자세하게 알아보도록 하겠습니다."
      ],
      "metadata": {
        "id": "o-mWFd85lc36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1-3. Attention Score**\n",
        "디코더에서는 지금 생성하려는 단어와 가장 연관성이 높은 인코더의 출력값을 찾아내기 위해 어텐션 스코어를 계산하게 됩니다.</br>\n",
        "디코더는 어텐션 스코어가 가장 높은 인코더의 출력값을 가장 많이 반영하여 새로운 단어를 생성합니다.</br>\n",
        "\n",
        "- **쿼리(Query), 키(Key), 벨류(Value)**</br>\n",
        "어텐션 스코어를 계산할 때 가장 중요한 개념은 쿼리, 키, 벨류 벡터가 무엇인지 이해하는 것입니다.</br>\n",
        "디코더는 **자신이 만들려는 단어(쿼리 벡터)**가 **기존의 단어들 중에서 어떤 것과 가장 비슷한지(키 벡터)** 확인하고 **그 단어의 값(벨류 벡터)**을 참고하여 새로운 단어를 생성합니다.</br>\n",
        "\n",
        "어텐션을 사용하는 Seq2Seq 모델의 쿼리, 키, 벨류 벡터는 다음과 같습니다.\n",
        "- 쿼리 : 디코더의 hidden state 벡터\n",
        "- 키 : 인코더의 hidden state 백터\n",
        "- 벨류 : 인코더의 hidden state 벡터"
      ],
      "metadata": {
        "id": "STUhYGt1pRPP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://i.imgur.com/gNcbamV.png\" title=\"source: imgur.com\" width=\"800\" /></a>"
      ],
      "metadata": {
        "id": "P5OfYEXkquQB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1-4. 모델 구현**\n",
        "기존의 Seq2Seq 모델에 어텐션 메커니즘을 추가해서 구현해보도록 하겠습니다."
      ],
      "metadata": {
        "id": "IL1wO6vIufda"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1) 인코더(Encoder)**\n",
        "하나의 컨텍스트 벡터를 생성했던 Seq2Seq 인코더와 다르게, 어텐션 인코더는 모든 인코더의 출력을 그대로 출력하도록 구현합니다."
      ],
      "metadata": {
        "id": "iLgQGMA4uq_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, units, vocab_size, embedding_dim, time_steps):\n",
        "        '''\n",
        "        Seq2Seq의 인코더입니다.\n",
        "\n",
        "        Args:\n",
        "            units (int) : 인코더 내부 lstm의 노드 수.\n",
        "            vocab_size (int) : 임베딩 행렬의 단어 수. 없는 단어가 있으면 oov가 발생할 수 있습니다.\n",
        "                -> 훈련하려는 문장의 단어는 모두 포함하고 있는 것이 좋다!\n",
        "            embedding_dim (int) : 임베딩 차원 수. 복잡할수록 좋을 수도 있고, 아닐 수도 있습니다. 차원이 크면 보통 표현력이 좋다. but 용량이 커져서 안좋을 수 있다.\n",
        "            time_steps (int) : 문장 토큰의 수. ex) 안녕하세요 조윤행입니다. -> 안녕하세요/ 조윤행/ 입니다/ -> 토큰 수 : 3개\n",
        "        '''\n",
        "        super().__init__()\n",
        "        self.embedding = Embedding(vocab_size,\n",
        "                                   embedding_dim,\n",
        "                                   input_length=time_steps)\n",
        "        self.dropout = Dropout(0.2)\n",
        "        # (attention) return_sequences=True 추가\n",
        "        self.lstm = LSTM(units,\n",
        "                         return_state=True,\n",
        "                         return_sequences=True\n",
        "                         )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.embedding(inputs)\n",
        "        x = self.dropout(x)\n",
        "        x, hidden_state, cell_state = self.lstm(x)\n",
        "        # (attention) x return 추가\n",
        "        return x, [hidden_state, cell_state]"
      ],
      "metadata": {
        "id": "8KlhFh3bvHRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2) 디코더(Decoder)**\n",
        " 디코더에서 모든 인코더 출력값에 대해 어텐션 스코어를 계산하여 반영하여 결과를 출력합니다."
      ],
      "metadata": {
        "id": "w6yZz1sRvfDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, units, vocab_size, embedding_dim, time_steps):\n",
        "        '''\n",
        "        Seq2Seq의 디코더입니다.\n",
        "\n",
        "        Args:\n",
        "            units (int) : 디코더 내부 lstm의 노드 수.\n",
        "            vocab_size (int) : 임베딩 행렬의 단어 수. 없는 단어가 있으면 oov가 발생할 수 있습니다.\n",
        "                -> 훈련하려는 문장의 단어는 모두 포함하고 있는 것이 좋다!\n",
        "            embedding_dim (int) : 임베딩 차원 수. 복잡할수록 좋을 수도 있고, 아닐 수도 있습니다. 차원이 크면 보통 표현력이 좋다. but 용량이 커져서 안좋을 수 있다.\n",
        "            time_steps (int) : 문장 토큰의 수. 디코더에서는 최대로 생성할 수 있는 문장의 길이가 됩니다.\n",
        "        '''\n",
        "        super().__init__()\n",
        "        self.embedding = Embedding(vocab_size,\n",
        "                                   embedding_dim,\n",
        "                                   input_length=time_steps)\n",
        "        self.dropout = Dropout(0.2)\n",
        "        self.lstm = LSTM(units,\n",
        "                         return_state=True,\n",
        "                         return_sequences=True)\n",
        "        #(attention) 어텐션 추가\n",
        "        self.attention = Attention()\n",
        "        self.dense = Dense(vocab_size, activation='softmax')\n",
        "\n",
        "    def call(self, inputs, initial_state):\n",
        "        # (attention) encoder_inputs 추가\n",
        "        encoder_inputs, decoder_inputs = inputs\n",
        "        x = self.embedding(decoder_inputs)\n",
        "        x = self.dropout(x)\n",
        "        x, hidden_state, cell_state = self.lstm(x, initial_state=initial_state)\n",
        "\n",
        "        # (attention) query_vector, attention_matrix 추가\n",
        "        # 이전 hidden_state의 값을 concat으로 만들어 query_vector를 생성합니다.\n",
        "        query_vector = tf.concat([initial_state[0][:, tf.newaxis, :],\n",
        "                               x[:, :-1, :]], axis=1)\n",
        "        # query_vector와 인코더에서 나온 출력 값들로 attention을 구합니다.\n",
        "        attention_matrix = self.attention([query_vector, encoder_inputs])\n",
        "        # 위에서 구한 attention_matrix와 decoder의 출력 값을 concat 합니다.\n",
        "        x = tf.concat([x, attention_matrix], axis=-1)\n",
        "\n",
        "        x = self.dense(x)\n",
        "        return x, hidden_state, cell_state"
      ],
      "metadata": {
        "id": "7IvcZLFqvz5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3) Seq2Seq with Attention**\n",
        "어텐션을 적용한 인코더와 디코더를 사용하여 seq2seq 모델을 구현합니다."
      ],
      "metadata": {
        "id": "sH0i2UHqxYk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2seq_with_Attention(tf.keras.Model):\n",
        "    def __init__(self, units, vocab_size, embedding_dim, time_steps, start_token, end_token):\n",
        "        \"\"\"\n",
        "        어텐션이 적용된 Seq2Seq 모델입니다. 인코더와 디코더를 선언합니다.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.start_token = start_token\n",
        "        self.end_token = end_token\n",
        "        self.time_steps = time_steps\n",
        "\n",
        "        self.encoder = Encoder(units, vocab_size, embedding_dim, time_steps)\n",
        "        self.decoder = Decoder(units, vocab_size, embedding_dim, time_steps)\n",
        "\n",
        "    def call(self, inputs, training=True):\n",
        "        \"\"\"\n",
        "        선언한 인코더와 디코더를 하나로 연결하여 Seq2Sqe with Attention 파이프라인을 구현합니다.\n",
        "\n",
        "        Args:\n",
        "            inputs : 문장의 단어 인덱스로 이루어진 데이터.\n",
        "            training : True인 경우 교사강요를 사용하여 디코더의 입력값에 정답을 넣어주며, False인 경우 디코더의 출력 데이터를 입력으로 넣어주게 됩니다.\n",
        "        \"\"\"\n",
        "        if training:\n",
        "            encoder_inputs, decoder_inputs = inputs\n",
        "            #(attention) 인코더가 context vector뿐만 아니라 모든 출력값을 만들도록 수정\n",
        "            encoder_outputs, context_vector = self.encoder(encoder_inputs)\n",
        "            #(attention) 디코더가 인코더의 모든 출력값을 받도록 수정\n",
        "            decoder_outputs, _, _ = self.decoder((encoder_outputs, decoder_inputs), context_vector)\n",
        "            return decoder_outputs\n",
        "        else:\n",
        "            #(attention) 인코더가 context vector뿐만 아니라 모든 출력값을 만들도록 수정\n",
        "            encoder_outputs, context_vector = self.encoder(inputs)\n",
        "            target_seq = tf.constant([[self.start_token]], dtype=tf.float32)\n",
        "            results = tf.TensorArray(tf.int32, self.time_steps)\n",
        "\n",
        "            for i in tf.range(self.time_steps):\n",
        "                #디코더가 인코더의 모든 출력값을 받도록 수정\n",
        "                decoder_output, decoder_hidden, decoder_cell = self.decoder((encoder_outputs, target_seq),\n",
        "                                                                            context_vector)\n",
        "                decoder_output = tf.cast(tf.argmax(decoder_output, axis= -1),\n",
        "                                         dtype=tf.int32)\n",
        "                decoder_output = tf.reshape(decoder_output, shape=(1, 1))\n",
        "                results = results.write(i, decoder_output)\n",
        "\n",
        "                if decoder_output == self.end_token:\n",
        "                    break\n",
        "\n",
        "                target_seq = decoder_output\n",
        "                context_vector = [decoder_hidden, decoder_cell]\n",
        "\n",
        "            return tf.reshape(results.stack(), shape=(1, self.time_steps))"
      ],
      "metadata": {
        "id": "TEeHyye6xWkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1-5. 챗봇 만들기**\n",
        "어텐션 메커니즘이 적용된 Seq2Seq 모델을 사용하여 챗봇을 구현해보도록 하겠습니다.</br>\n",
        "(과정은 기존 Seq2Seq와 동일하므로 데이터로드 및 전처리, 토큰화 과정은 생략하겠습니다.)"
      ],
      "metadata": {
        "id": "GzUvv9ZZymCl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1) 모델 훈련**"
      ],
      "metadata": {
        "id": "6NNMn9MUy-ow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 훈련을 위해 모델을 선언합니다.\n",
        "seq2seq_a = Seq2seq_with_Attention(UNITS,\n",
        "                  VOCAB_SIZE,\n",
        "                  EMBEDDING_DIM,\n",
        "                  TIME_STEPS,\n",
        "                  START_TOKEN,\n",
        "                  END_TOKEN)\n",
        "\n",
        "seq2seq_a.compile(optimizer='adam',\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['acc'])"
      ],
      "metadata": {
        "id": "O9h-Wy20zK7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#학습된 모델을 저장하기위해 체크포인트를 지정합니다.\n",
        "checkpoint_path = 'model/seq2seq_with_Attention.ckpt'\n",
        "checkpoint = ModelCheckpoint(filepath=checkpoint_path,\n",
        "                             save_weights_only= True,\n",
        "                             save_best_only=True,\n",
        "                             monitor='loss',\n",
        "                             verbose=1\n",
        "                             )"
      ],
      "metadata": {
        "id": "6NOdgps1zUkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 훈련\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f'processing epoch: {epoch * 10 + 1}...')\n",
        "    seq2seq_a.fit([question_pad, answer_in_pad],\n",
        "                answer_out_pad,\n",
        "                epochs=10,\n",
        "                batch_size=BATCH_SIZE,\n",
        "                callbacks=[checkpoint]\n",
        "               )\n",
        "    # 랜덤한 샘플 번호 추출\n",
        "    samples = np.random.randint(DATA_LENGTH, size=SAMPLE_SIZE)\n",
        "\n",
        "    # 예측 성능 테스트\n",
        "    for idx in samples:\n",
        "        question_inputs = question_pad[idx]\n",
        "        # 문장 예측\n",
        "        results = make_prediction(seq2seq_a, np.expand_dims(question_inputs, 0))\n",
        "\n",
        "        # 변환된 인덱스를 문장으로 변환\n",
        "        results = convert_index_to_text(results, END_TOKEN)\n",
        "\n",
        "        print(f'Q: {questions[idx]}')\n",
        "        print(f'A: {results}\\n')\n",
        "        print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXXX4stjzmUx",
        "outputId": "aa950c32-6acd-427b-90d3-7b6036d4e566"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing epoch: 1...\n",
            "Epoch 1/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 2.7165 - acc: 0.7043\n",
            "Epoch 1: loss improved from inf to 2.70026, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 7s 13ms/step - loss: 2.7003 - acc: 0.7039\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 1.7135 - acc: 0.7488\n",
            "Epoch 2: loss improved from 2.70026 to 1.71346, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 1.7135 - acc: 0.7488\n",
            "Epoch 3/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 1.6044 - acc: 0.7597\n",
            "Epoch 3: loss improved from 1.71346 to 1.60460, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 1.6046 - acc: 0.7597\n",
            "Epoch 4/10\n",
            "121/126 [===========================>..] - ETA: 0s - loss: 1.5394 - acc: 0.7611\n",
            "Epoch 4: loss improved from 1.60460 to 1.53602, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 1.5360 - acc: 0.7618\n",
            "Epoch 5/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 1.4857 - acc: 0.7662\n",
            "Epoch 5: loss improved from 1.53602 to 1.48408, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 1.4841 - acc: 0.7665\n",
            "Epoch 6/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 1.4408 - acc: 0.7699\n",
            "Epoch 6: loss improved from 1.48408 to 1.44081, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 1.4408 - acc: 0.7699\n",
            "Epoch 7/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 1.4010 - acc: 0.7723\n",
            "Epoch 7: loss improved from 1.44081 to 1.40144, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 1.4014 - acc: 0.7722\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 1.3620 - acc: 0.7759\n",
            "Epoch 8: loss improved from 1.40144 to 1.36204, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 1.3620 - acc: 0.7759\n",
            "Epoch 9/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 1.3268 - acc: 0.7768\n",
            "Epoch 9: loss improved from 1.36204 to 1.32687, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 1.3269 - acc: 0.7768\n",
            "Epoch 10/10\n",
            "121/126 [===========================>..] - ETA: 0s - loss: 1.2887 - acc: 0.7815\n",
            "Epoch 10: loss improved from 1.32687 to 1.28816, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 1.2882 - acc: 0.7814\n",
            "Q: 놀이 공원 데이트 어때 ?\n",
            "A: 저 을 마음 을 사람 을 사람 이 사람 이 자신감 을 가져 보세요 \n",
            "\n",
            "\n",
            "Q: 말투 가 별로 야\n",
            "A: 저 을 마음 을 사람 을 사람 이 사람 이 자신감 을 가져 보세요 \n",
            "\n",
            "\n",
            "Q: 남편 이 짜증나게해\n",
            "A: 저 을 마음 을 사람 을 사람 이 사람 이 자신감 을 가져 보세요 \n",
            "\n",
            "\n",
            "Q: 모르겠는데 노력 하래\n",
            "A: 저 을 마음 을 사람 을 사람 이 사람 이 자신감 을 가져 보세요 \n",
            "\n",
            "\n",
            "Q: 날씨 가 진짜 덥다\n",
            "A: 저 을 마음 을 사람 을 사람 이 사람 이 자신감 을 가져 보세요 \n",
            "\n",
            "\n",
            "processing epoch: 11...\n",
            "Epoch 1/10\n",
            "121/126 [===========================>..] - ETA: 0s - loss: 1.2470 - acc: 0.7850\n",
            "Epoch 1: loss improved from 1.28816 to 1.24747, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 1.2475 - acc: 0.7849\n",
            "Epoch 2/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 1.2082 - acc: 0.7893\n",
            "Epoch 2: loss improved from 1.24747 to 1.20824, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 1.2082 - acc: 0.7893\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 1.1679 - acc: 0.7931\n",
            "Epoch 3: loss improved from 1.20824 to 1.16790, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 1.1679 - acc: 0.7931\n",
            "Epoch 4/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 1.1265 - acc: 0.7984\n",
            "Epoch 4: loss improved from 1.16790 to 1.12663, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 1.1266 - acc: 0.7984\n",
            "Epoch 5/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 1.0842 - acc: 0.8022\n",
            "Epoch 5: loss improved from 1.12663 to 1.08362, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 2s 13ms/step - loss: 1.0836 - acc: 0.8025\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 1.0403 - acc: 0.8070\n",
            "Epoch 6: loss improved from 1.08362 to 1.04034, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 1.0403 - acc: 0.8070\n",
            "Epoch 7/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.9967 - acc: 0.8134\n",
            "Epoch 7: loss improved from 1.04034 to 0.99643, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.9964 - acc: 0.8134\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.9529 - acc: 0.8193\n",
            "Epoch 8: loss improved from 0.99643 to 0.95288, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.9529 - acc: 0.8193\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.9136 - acc: 0.8254\n",
            "Epoch 9: loss improved from 0.95288 to 0.91362, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.9136 - acc: 0.8254\n",
            "Epoch 10/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.8708 - acc: 0.8335\n",
            "Epoch 10: loss improved from 0.91362 to 0.87229, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.8723 - acc: 0.8333\n",
            "Q: 그냥 공무원 이 좋을 듯\n",
            "A: 저 도 듣고 싶어요 \n",
            "\n",
            "\n",
            "Q: 뭐 해 ?\n",
            "A: 저 도 듣고 싶네요 \n",
            "\n",
            "\n",
            "Q: 바다 가자고 하면 갈까 ?\n",
            "A: 저 도 듣고 싶네요 \n",
            "\n",
            "\n",
            "Q: 목 이 뻑뻑 해\n",
            "A: 저 도 듣고 싶네요 \n",
            "\n",
            "\n",
            "Q: 모르는 사람 이 자꾸 쳐다봐\n",
            "A: 저 도 듣고 싶네요 \n",
            "\n",
            "\n",
            "processing epoch: 21...\n",
            "Epoch 1/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.8363 - acc: 0.8387\n",
            "Epoch 1: loss improved from 0.87229 to 0.83458, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.8346 - acc: 0.8392\n",
            "Epoch 2/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.7956 - acc: 0.8468\n",
            "Epoch 2: loss improved from 0.83458 to 0.79561, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 2s 16ms/step - loss: 0.7956 - acc: 0.8468\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.7589 - acc: 0.8551\n",
            "Epoch 3: loss improved from 0.79561 to 0.75891, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 2s 13ms/step - loss: 0.7589 - acc: 0.8551\n",
            "Epoch 4/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.7256 - acc: 0.8620\n",
            "Epoch 4: loss improved from 0.75891 to 0.72540, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.7254 - acc: 0.8621\n",
            "Epoch 5/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.6948 - acc: 0.8675\n",
            "Epoch 5: loss improved from 0.72540 to 0.69486, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.6949 - acc: 0.8676\n",
            "Epoch 6/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.6611 - acc: 0.8727\n",
            "Epoch 6: loss improved from 0.69486 to 0.66206, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.6621 - acc: 0.8725\n",
            "Epoch 7/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.6317 - acc: 0.8808\n",
            "Epoch 7: loss improved from 0.66206 to 0.63230, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.6323 - acc: 0.8805\n",
            "Epoch 8/10\n",
            "121/126 [===========================>..] - ETA: 0s - loss: 0.6006 - acc: 0.8870\n",
            "Epoch 8: loss improved from 0.63230 to 0.60029, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.6003 - acc: 0.8869\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.5766 - acc: 0.8908\n",
            "Epoch 9: loss improved from 0.60029 to 0.57661, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.5766 - acc: 0.8908\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.5524 - acc: 0.8953\n",
            "Epoch 10: loss improved from 0.57661 to 0.55238, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.5524 - acc: 0.8953\n",
            "Q: 나 만 일시 켜서 짜증 폭발\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "Q: 공시 준비 중\n",
            "A: 잘 할 수 있을 거 예요 \n",
            "\n",
            "\n",
            "Q: 공부 하기 싫다\n",
            "A: 자신 의 눈높이 가 맞는 사람 만나세요 \n",
            "\n",
            "\n",
            "Q: 가족 들 이랑 서먹해\n",
            "A: 자신 의 독특함을 믿으세요 \n",
            "\n",
            "\n",
            "Q: 남자친구 가 잔소리 가 심해\n",
            "A: 저 는 위로 봇 입니다 \n",
            "\n",
            "\n",
            "processing epoch: 31...\n",
            "Epoch 1/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.5255 - acc: 0.9021\n",
            "Epoch 1: loss improved from 0.55238 to 0.52566, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.5257 - acc: 0.9021\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.5021 - acc: 0.9059\n",
            "Epoch 2: loss improved from 0.52566 to 0.50214, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.5021 - acc: 0.9059\n",
            "Epoch 3/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.4833 - acc: 0.9089\n",
            "Epoch 3: loss improved from 0.50214 to 0.48334, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.4833 - acc: 0.9089\n",
            "Epoch 4/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.4622 - acc: 0.9132\n",
            "Epoch 4: loss improved from 0.48334 to 0.46255, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.4625 - acc: 0.9130\n",
            "Epoch 5/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.4446 - acc: 0.9153\n",
            "Epoch 5: loss improved from 0.46255 to 0.44512, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.4451 - acc: 0.9152\n",
            "Epoch 6/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.4385 - acc: 0.9169\n",
            "Epoch 6: loss improved from 0.44512 to 0.43813, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.4381 - acc: 0.9170\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.4132 - acc: 0.9207\n",
            "Epoch 7: loss improved from 0.43813 to 0.41318, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.4132 - acc: 0.9207\n",
            "Epoch 8/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.3966 - acc: 0.9233\n",
            "Epoch 8: loss improved from 0.41318 to 0.39619, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.3962 - acc: 0.9233\n",
            "Epoch 9/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.3818 - acc: 0.9263\n",
            "Epoch 9: loss improved from 0.39619 to 0.38112, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.3811 - acc: 0.9267\n",
            "Epoch 10/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.3678 - acc: 0.9288\n",
            "Epoch 10: loss improved from 0.38112 to 0.36809, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.3681 - acc: 0.9286\n",
            "Q: 나도 집 사고 싶어\n",
            "A: 제 가 있잖아요 \n",
            "\n",
            "\n",
            "Q: 돌겠다\n",
            "A: 저 랑 놀아요 \n",
            "\n",
            "\n",
            "Q: 돈 빌려 줬는데 연락 두절\n",
            "A: 맛있게 드세요 \n",
            "\n",
            "\n",
            "Q: 가스 비 장난 아님\n",
            "A: 잘 할 수 있을 거 예요 \n",
            "\n",
            "\n",
            "Q: 내 가 그렇게 부족한가\n",
            "A: 마음 이 아픈가요 \n",
            "\n",
            "\n",
            "processing epoch: 41...\n",
            "Epoch 1/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.3551 - acc: 0.9301\n",
            "Epoch 1: loss improved from 0.36809 to 0.35524, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 2s 12ms/step - loss: 0.3552 - acc: 0.9301\n",
            "Epoch 2/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.3458 - acc: 0.9316\n",
            "Epoch 2: loss improved from 0.35524 to 0.34584, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.3458 - acc: 0.9316\n",
            "Epoch 3/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.3351 - acc: 0.9332\n",
            "Epoch 3: loss improved from 0.34584 to 0.33556, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.3356 - acc: 0.9331\n",
            "Epoch 4/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.3247 - acc: 0.9342\n",
            "Epoch 4: loss improved from 0.33556 to 0.32468, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.3247 - acc: 0.9342\n",
            "Epoch 5/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.3181 - acc: 0.9355\n",
            "Epoch 5: loss improved from 0.32468 to 0.31855, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.3185 - acc: 0.9354\n",
            "Epoch 6/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.3057 - acc: 0.9375\n",
            "Epoch 6: loss improved from 0.31855 to 0.30591, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.3059 - acc: 0.9375\n",
            "Epoch 7/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.2966 - acc: 0.9379\n",
            "Epoch 7: loss improved from 0.30591 to 0.29709, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.2971 - acc: 0.9378\n",
            "Epoch 8/10\n",
            "121/126 [===========================>..] - ETA: 0s - loss: 0.2913 - acc: 0.9392\n",
            "Epoch 8: loss improved from 0.29709 to 0.29106, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.2911 - acc: 0.9391\n",
            "Epoch 9/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.2885 - acc: 0.9393\n",
            "Epoch 9: loss improved from 0.29106 to 0.28848, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.2885 - acc: 0.9393\n",
            "Epoch 10/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.2808 - acc: 0.9405\n",
            "Epoch 10: loss improved from 0.28848 to 0.28084, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.2808 - acc: 0.9406\n",
            "Q: 나 챙겨줄 사람 이 필요해\n",
            "A: 저 도 좋아해요 \n",
            "\n",
            "\n",
            "Q: 뒤 돌아 보지 말고 나가야 하는데\n",
            "A: 글 쓰면서 정리 가 될 거 예요 \n",
            "\n",
            "\n",
            "Q: 그냥 이렇게 살 고 싶어\n",
            "A: 돈 없어도 할 수 있는 건 큰 행운 일 거 예요 \n",
            "\n",
            "\n",
            "Q: 땅 이나 살까\n",
            "A: 잘 할 수 있을 거 예요 \n",
            "\n",
            "\n",
            "Q: 농담 처럼 진담 하는 사람\n",
            "A: 마음 으로 충분할 거 예요 \n",
            "\n",
            "\n",
            "processing epoch: 51...\n",
            "Epoch 1/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.2695 - acc: 0.9419\n",
            "Epoch 1: loss improved from 0.28084 to 0.26941, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.2694 - acc: 0.9419\n",
            "Epoch 2/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.2621 - acc: 0.9433\n",
            "Epoch 2: loss improved from 0.26941 to 0.26211, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.2621 - acc: 0.9433\n",
            "Epoch 3/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.2554 - acc: 0.9433\n",
            "Epoch 3: loss improved from 0.26211 to 0.25594, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.2559 - acc: 0.9433\n",
            "Epoch 4/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.2499 - acc: 0.9445\n",
            "Epoch 4: loss improved from 0.25594 to 0.25021, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.2502 - acc: 0.9444\n",
            "Epoch 5/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.2443 - acc: 0.9453\n",
            "Epoch 5: loss improved from 0.25021 to 0.24419, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.2442 - acc: 0.9454\n",
            "Epoch 6/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.2368 - acc: 0.9463\n",
            "Epoch 6: loss improved from 0.24419 to 0.23685, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.2368 - acc: 0.9463\n",
            "Epoch 7/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.2339 - acc: 0.9470\n",
            "Epoch 7: loss improved from 0.23685 to 0.23397, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.2340 - acc: 0.9470\n",
            "Epoch 8/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.2312 - acc: 0.9472\n",
            "Epoch 8: loss improved from 0.23397 to 0.23106, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.2311 - acc: 0.9472\n",
            "Epoch 9/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.2250 - acc: 0.9480\n",
            "Epoch 9: loss improved from 0.23106 to 0.22567, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.2257 - acc: 0.9480\n",
            "Epoch 10/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.2194 - acc: 0.9492\n",
            "Epoch 10: loss improved from 0.22567 to 0.21976, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.2198 - acc: 0.9492\n",
            "Q: 누구 냐 넌 ?\n",
            "A: 다시 방문 해보세요 \n",
            "\n",
            "\n",
            "Q: 동호회 나가지 말까\n",
            "A: 잠시 눈 을 붙 여보세요 \n",
            "\n",
            "\n",
            "Q: 뭐 가 잘 못 된 걸까\n",
            "A: 잠시 눈 을 붙 여보세요 \n",
            "\n",
            "\n",
            "Q: 녹차 한 잔 어때 ?\n",
            "A: 저 랑 한 잔 해 요 \n",
            "\n",
            "\n",
            "Q: 명품 선물 부담스러울까\n",
            "A: 자신 의 눈높이 가 맞는 사람 만나세요 \n",
            "\n",
            "\n",
            "processing epoch: 61...\n",
            "Epoch 1/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.2151 - acc: 0.9497\n",
            "Epoch 1: loss improved from 0.21976 to 0.21529, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.2153 - acc: 0.9496\n",
            "Epoch 2/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.2100 - acc: 0.9500\n",
            "Epoch 2: loss improved from 0.21529 to 0.21000, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.2100 - acc: 0.9500\n",
            "Epoch 3/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.2035 - acc: 0.9509\n",
            "Epoch 3: loss improved from 0.21000 to 0.20337, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 2s 12ms/step - loss: 0.2034 - acc: 0.9509\n",
            "Epoch 4/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.1997 - acc: 0.9520\n",
            "Epoch 4: loss improved from 0.20337 to 0.19973, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.1997 - acc: 0.9519\n",
            "Epoch 5/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.1945 - acc: 0.9529\n",
            "Epoch 5: loss improved from 0.19973 to 0.19432, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.1943 - acc: 0.9528\n",
            "Epoch 6/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.1889 - acc: 0.9539\n",
            "Epoch 6: loss improved from 0.19432 to 0.18930, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.1893 - acc: 0.9538\n",
            "Epoch 7/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.1856 - acc: 0.9542\n",
            "Epoch 7: loss improved from 0.18930 to 0.18562, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 2s 13ms/step - loss: 0.1856 - acc: 0.9542\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.1786 - acc: 0.9553\n",
            "Epoch 8: loss improved from 0.18562 to 0.17861, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 2s 13ms/step - loss: 0.1786 - acc: 0.9553\n",
            "Epoch 9/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.1747 - acc: 0.9561\n",
            "Epoch 9: loss improved from 0.17861 to 0.17481, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.1748 - acc: 0.9560\n",
            "Epoch 10/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.1751 - acc: 0.9559\n",
            "Epoch 10: loss did not improve from 0.17481\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.1751 - acc: 0.9559\n",
            "Q: 나중 에 뭐 하고 먹고 사냐\n",
            "A: 그래서 저 는 못 기르고 잘라요 \n",
            "\n",
            "\n",
            "Q: 고 3 이니까 공부 해야겠지\n",
            "A: 공부 가 최 우선 이 죠 \n",
            "\n",
            "\n",
            "Q: 목욕탕 가야 지\n",
            "A: 먼저 생활 패턴 을 살펴 보세요 \n",
            "\n",
            "\n",
            "Q: 땅 사면 좋겠다\n",
            "A: 잘 되길 바랍니다 \n",
            "\n",
            "\n",
            "Q: 꽃 선물 받고 어\n",
            "A: 제 가 드리고 싶네요 \n",
            "\n",
            "\n",
            "processing epoch: 71...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.1698 - acc: 0.9568\n",
            "Epoch 1: loss improved from 0.17481 to 0.16984, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.1698 - acc: 0.9568\n",
            "Epoch 2/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.1633 - acc: 0.9584\n",
            "Epoch 2: loss improved from 0.16984 to 0.16316, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.1632 - acc: 0.9585\n",
            "Epoch 3/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.1598 - acc: 0.9599\n",
            "Epoch 3: loss improved from 0.16316 to 0.15982, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.1598 - acc: 0.9599\n",
            "Epoch 4/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.1548 - acc: 0.9595\n",
            "Epoch 4: loss improved from 0.15982 to 0.15459, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.1546 - acc: 0.9594\n",
            "Epoch 5/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.1484 - acc: 0.9621\n",
            "Epoch 5: loss improved from 0.15459 to 0.14898, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 2s 12ms/step - loss: 0.1490 - acc: 0.9620\n",
            "Epoch 6/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.1453 - acc: 0.9620\n",
            "Epoch 6: loss improved from 0.14898 to 0.14552, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.1455 - acc: 0.9618\n",
            "Epoch 7/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.1394 - acc: 0.9641\n",
            "Epoch 7: loss improved from 0.14552 to 0.13922, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.1392 - acc: 0.9641\n",
            "Epoch 8/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.1371 - acc: 0.9634\n",
            "Epoch 8: loss improved from 0.13922 to 0.13710, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.1371 - acc: 0.9635\n",
            "Epoch 9/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.1315 - acc: 0.9652\n",
            "Epoch 9: loss improved from 0.13710 to 0.13166, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 12ms/step - loss: 0.1317 - acc: 0.9651\n",
            "Epoch 10/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.1275 - acc: 0.9668\n",
            "Epoch 10: loss improved from 0.13166 to 0.12773, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.1277 - acc: 0.9667\n",
            "Q: 바다 가자고 하면 갈까 ?\n",
            "A: 같이 가자고 말 해보세요 \n",
            "\n",
            "\n",
            "Q: 만족 을 모르나 봐\n",
            "A: 만족하면 발전 이 없으니까 요 \n",
            "\n",
            "\n",
            "Q: 단순하다\n",
            "A: 회사 와 자신 에 대해 서 더 공부 해서 자신감 을 가져 보세요 \n",
            "\n",
            "\n",
            "Q: 꽃놀이 가고 싶어\n",
            "A: 운전 재미있어요 \n",
            "\n",
            "\n",
            "Q: 밤 에 잠 이 안 와\n",
            "A: 운동 을 해보세요 \n",
            "\n",
            "\n",
            "processing epoch: 81...\n",
            "Epoch 1/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.1267 - acc: 0.9661\n",
            "Epoch 1: loss improved from 0.12773 to 0.12670, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.1267 - acc: 0.9661\n",
            "Epoch 2/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.1220 - acc: 0.9675\n",
            "Epoch 2: loss improved from 0.12670 to 0.12246, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.1225 - acc: 0.9674\n",
            "Epoch 3/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.1156 - acc: 0.9688\n",
            "Epoch 3: loss improved from 0.12246 to 0.11588, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.1159 - acc: 0.9687\n",
            "Epoch 4/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.1175 - acc: 0.9684\n",
            "Epoch 4: loss did not improve from 0.11588\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.1176 - acc: 0.9684\n",
            "Epoch 5/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.1166 - acc: 0.9688\n",
            "Epoch 5: loss did not improve from 0.11588\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.1165 - acc: 0.9689\n",
            "Epoch 6/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.1072 - acc: 0.9712\n",
            "Epoch 6: loss improved from 0.11588 to 0.10714, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.1071 - acc: 0.9712\n",
            "Epoch 7/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.1011 - acc: 0.9736\n",
            "Epoch 7: loss improved from 0.10714 to 0.10100, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.1010 - acc: 0.9736\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.1006 - acc: 0.9730\n",
            "Epoch 8: loss improved from 0.10100 to 0.10064, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.1006 - acc: 0.9730\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0974 - acc: 0.9739\n",
            "Epoch 9: loss improved from 0.10064 to 0.09744, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0974 - acc: 0.9739\n",
            "Epoch 10/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0903 - acc: 0.9763\n",
            "Epoch 10: loss improved from 0.09744 to 0.09038, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 12ms/step - loss: 0.0904 - acc: 0.9763\n",
            "Q: 가출 해도 갈 데 가 없어\n",
            "A: 선생님 이나 기관 에 연락 해보세요 \n",
            "\n",
            "\n",
            "Q: 무한리필 괜찮지 않아 ?\n",
            "A: 양 이 많으면 추천 해요 \n",
            "\n",
            "\n",
            "Q: 고구마 다이어트 해야지\n",
            "A: 너무 무리하면 지쳐요 \n",
            "\n",
            "\n",
            "Q: 고구마 다이어트 해야지\n",
            "A: 너무 무리하면 지쳐요 \n",
            "\n",
            "\n",
            "Q: 건물 주 되고싶어\n",
            "A: 거꾸로 해서 드라이 플라워 만들어 보세요 \n",
            "\n",
            "\n",
            "processing epoch: 91...\n",
            "Epoch 1/10\n",
            "121/126 [===========================>..] - ETA: 0s - loss: 0.0886 - acc: 0.9772\n",
            "Epoch 1: loss improved from 0.09038 to 0.08910, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0891 - acc: 0.9770\n",
            "Epoch 2/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0851 - acc: 0.9766\n",
            "Epoch 2: loss improved from 0.08910 to 0.08513, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0851 - acc: 0.9766\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0817 - acc: 0.9784\n",
            "Epoch 3: loss improved from 0.08513 to 0.08175, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0817 - acc: 0.9784\n",
            "Epoch 4/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0799 - acc: 0.9794\n",
            "Epoch 4: loss improved from 0.08175 to 0.08010, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0801 - acc: 0.9793\n",
            "Epoch 5/10\n",
            "121/126 [===========================>..] - ETA: 0s - loss: 0.0765 - acc: 0.9802\n",
            "Epoch 5: loss improved from 0.08010 to 0.07679, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0768 - acc: 0.9800\n",
            "Epoch 6/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0757 - acc: 0.9802\n",
            "Epoch 6: loss improved from 0.07679 to 0.07569, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0757 - acc: 0.9802\n",
            "Epoch 7/10\n",
            "121/126 [===========================>..] - ETA: 0s - loss: 0.0707 - acc: 0.9812\n",
            "Epoch 7: loss improved from 0.07569 to 0.07079, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0708 - acc: 0.9813\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0679 - acc: 0.9820\n",
            "Epoch 8: loss improved from 0.07079 to 0.06788, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0679 - acc: 0.9820\n",
            "Epoch 9/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0662 - acc: 0.9825\n",
            "Epoch 9: loss improved from 0.06788 to 0.06620, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0662 - acc: 0.9825\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0649 - acc: 0.9828\n",
            "Epoch 10: loss improved from 0.06620 to 0.06493, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0649 - acc: 0.9828\n",
            "Q: 마음 이 울적해\n",
            "A: 소리 를 크게 한번 질러 보세요 \n",
            "\n",
            "\n",
            "Q: 내 가 이상한 건가 ?\n",
            "A: 자신 의 독특함을 믿으세요 \n",
            "\n",
            "\n",
            "Q: 롱패딩 필요할까\n",
            "A: 추울 때 딱이죠 \n",
            "\n",
            "\n",
            "Q: 나 서류 에서 광탈 했어\n",
            "A: 자책 하지 마세요 \n",
            "\n",
            "\n",
            "Q: 무서워요\n",
            "A: 어흥 호랑이 보다 무섭나요 ? \n",
            "\n",
            "\n",
            "processing epoch: 101...\n",
            "Epoch 1/10\n",
            "121/126 [===========================>..] - ETA: 0s - loss: 0.0603 - acc: 0.9843\n",
            "Epoch 1: loss improved from 0.06493 to 0.06037, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0604 - acc: 0.9842\n",
            "Epoch 2/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9843\n",
            "Epoch 2: loss did not improve from 0.06037\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.0607 - acc: 0.9843\n",
            "Epoch 3/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0560 - acc: 0.9855\n",
            "Epoch 3: loss improved from 0.06037 to 0.05608, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0561 - acc: 0.9855\n",
            "Epoch 4/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0563 - acc: 0.9859\n",
            "Epoch 4: loss did not improve from 0.05608\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.0566 - acc: 0.9859\n",
            "Epoch 5/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0527 - acc: 0.9869\n",
            "Epoch 5: loss improved from 0.05608 to 0.05272, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0527 - acc: 0.9869\n",
            "Epoch 6/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0551 - acc: 0.9850\n",
            "Epoch 6: loss did not improve from 0.05272\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0551 - acc: 0.9850\n",
            "Epoch 7/10\n",
            "121/126 [===========================>..] - ETA: 0s - loss: 0.0476 - acc: 0.9884\n",
            "Epoch 7: loss improved from 0.05272 to 0.04814, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0481 - acc: 0.9883\n",
            "Epoch 8/10\n",
            "121/126 [===========================>..] - ETA: 0s - loss: 0.0472 - acc: 0.9881\n",
            "Epoch 8: loss improved from 0.04814 to 0.04747, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0475 - acc: 0.9880\n",
            "Epoch 9/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9886\n",
            "Epoch 9: loss improved from 0.04747 to 0.04617, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 12ms/step - loss: 0.0462 - acc: 0.9885\n",
            "Epoch 10/10\n",
            "121/126 [===========================>..] - ETA: 0s - loss: 0.0446 - acc: 0.9897\n",
            "Epoch 10: loss improved from 0.04617 to 0.04463, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0446 - acc: 0.9896\n",
            "Q: 내 가 왜 해야하는지 모르겠어\n",
            "A: 그 사람 도 설렐 거 예요 \n",
            "\n",
            "\n",
            "Q: 뭔가 자꾸 불안해\n",
            "A: 두려워하지 않아도 돼요 \n",
            "\n",
            "\n",
            "Q: 내 스타일 아니던데\n",
            "A: 새로운 스타일 도전 해 보시 면 어때요 ? \n",
            "\n",
            "\n",
            "Q: 머리 냄새 날텐데\n",
            "A: 항상 청결하게 관리 하세요 \n",
            "\n",
            "\n",
            "Q: 기분 꿀꿀해\n",
            "A: 내일 은 오늘 보다 나을 거 예요 \n",
            "\n",
            "\n",
            "processing epoch: 111...\n",
            "Epoch 1/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9902\n",
            "Epoch 1: loss improved from 0.04463 to 0.04028, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 2s 13ms/step - loss: 0.0403 - acc: 0.9902\n",
            "Epoch 2/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0426 - acc: 0.9894\n",
            "Epoch 2: loss did not improve from 0.04028\n",
            "126/126 [==============================] - 1s 12ms/step - loss: 0.0425 - acc: 0.9894\n",
            "Epoch 3/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9909\n",
            "Epoch 3: loss improved from 0.04028 to 0.03856, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.0386 - acc: 0.9909\n",
            "Epoch 4/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9918\n",
            "Epoch 4: loss improved from 0.03856 to 0.03634, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 2s 13ms/step - loss: 0.0363 - acc: 0.9918\n",
            "Epoch 5/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9922\n",
            "Epoch 5: loss improved from 0.03634 to 0.03428, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0343 - acc: 0.9922\n",
            "Epoch 6/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9919\n",
            "Epoch 6: loss did not improve from 0.03428\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0347 - acc: 0.9919\n",
            "Epoch 7/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9894\n",
            "Epoch 7: loss did not improve from 0.03428\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.0434 - acc: 0.9894\n",
            "Epoch 8/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9917\n",
            "Epoch 8: loss did not improve from 0.03428\n",
            "126/126 [==============================] - 1s 9ms/step - loss: 0.0363 - acc: 0.9917\n",
            "Epoch 9/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9931\n",
            "Epoch 9: loss improved from 0.03428 to 0.03120, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0312 - acc: 0.9932\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0304 - acc: 0.9932\n",
            "Epoch 10: loss improved from 0.03120 to 0.03037, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0304 - acc: 0.9932\n",
            "Q: 남편 이 짜증나게해\n",
            "A: 처음 만났을 때 를 떠올려 보세요 \n",
            "\n",
            "\n",
            "Q: 너무 배 가 불러\n",
            "A: 좀 쉬세요 \n",
            "\n",
            "\n",
            "Q: 단발 해볼까 ?\n",
            "A: 변화 를 주는 것 도 좋겠죠 \n",
            "\n",
            "\n",
            "Q: 남자 인지 여자 인지 알려줘\n",
            "A: 아직 모르겠어요 인공 지능 에 성별 을 만드는 사람 이 되어 주세요 \n",
            "\n",
            "\n",
            "Q: 나 천 재임\n",
            "A: 제 가 따라가려면 멀었네요 \n",
            "\n",
            "\n",
            "processing epoch: 121...\n",
            "Epoch 1/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9935\n",
            "Epoch 1: loss improved from 0.03037 to 0.02850, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0285 - acc: 0.9935\n",
            "Epoch 2/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9948\n",
            "Epoch 2: loss improved from 0.02850 to 0.02486, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0249 - acc: 0.9948\n",
            "Epoch 3/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.0236 - acc: 0.9953\n",
            "Epoch 3: loss improved from 0.02486 to 0.02374, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0237 - acc: 0.9954\n",
            "Epoch 4/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9941\n",
            "Epoch 4: loss did not improve from 0.02374\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0277 - acc: 0.9939\n",
            "Epoch 5/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9944\n",
            "Epoch 5: loss did not improve from 0.02374\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0254 - acc: 0.9945\n",
            "Epoch 6/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.0225 - acc: 0.9951\n",
            "Epoch 6: loss improved from 0.02374 to 0.02234, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0223 - acc: 0.9951\n",
            "Epoch 7/10\n",
            "121/126 [===========================>..] - ETA: 0s - loss: 0.0196 - acc: 0.9963\n",
            "Epoch 7: loss improved from 0.02234 to 0.01952, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0195 - acc: 0.9963\n",
            "Epoch 8/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0199 - acc: 0.9955\n",
            "Epoch 8: loss did not improve from 0.01952\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0199 - acc: 0.9955\n",
            "Epoch 9/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9954\n",
            "Epoch 9: loss did not improve from 0.01952\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0211 - acc: 0.9954\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0185 - acc: 0.9962\n",
            "Epoch 10: loss improved from 0.01952 to 0.01850, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0185 - acc: 0.9962\n",
            "Q: 또 개강 이야\n",
            "A: 즐거운 시작 되길 바랍니다 \n",
            "\n",
            "\n",
            "Q: 기분 나쁜 농담 을 계속 하고 있어\n",
            "A: 정색 한번 해주세요 \n",
            "\n",
            "\n",
            "Q: 뭔가 뒷맛 이 쓰다\n",
            "A: 단맛 도 있을 거 예요 \n",
            "\n",
            "\n",
            "Q: 눈 또 와\n",
            "A: 눈 내리는게 싫은가 봐요 \n",
            "\n",
            "\n",
            "Q: 나 왕따 인거 같아\n",
            "A: 부모님 께 도움 을 청 해보세요 \n",
            "\n",
            "\n",
            "processing epoch: 131...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0174 - acc: 0.9968\n",
            "Epoch 1: loss improved from 0.01850 to 0.01743, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0174 - acc: 0.9968\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0177 - acc: 0.9963\n",
            "Epoch 2: loss did not improve from 0.01743\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0177 - acc: 0.9963\n",
            "Epoch 3/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0179 - acc: 0.9960\n",
            "Epoch 3: loss did not improve from 0.01743\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0180 - acc: 0.9960\n",
            "Epoch 4/10\n",
            "121/126 [===========================>..] - ETA: 0s - loss: 0.0151 - acc: 0.9971\n",
            "Epoch 4: loss improved from 0.01743 to 0.01535, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 2s 12ms/step - loss: 0.0154 - acc: 0.9970\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0163 - acc: 0.9967\n",
            "Epoch 5: loss did not improve from 0.01535\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0163 - acc: 0.9967\n",
            "Epoch 6/10\n",
            "121/126 [===========================>..] - ETA: 0s - loss: 0.0179 - acc: 0.9956\n",
            "Epoch 6: loss did not improve from 0.01535\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0181 - acc: 0.9955\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0170 - acc: 0.9961\n",
            "Epoch 7: loss did not improve from 0.01535\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0170 - acc: 0.9961\n",
            "Epoch 8/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9957\n",
            "Epoch 8: loss did not improve from 0.01535\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0189 - acc: 0.9957\n",
            "Epoch 9/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.0173 - acc: 0.9964\n",
            "Epoch 9: loss did not improve from 0.01535\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0173 - acc: 0.9964\n",
            "Epoch 10/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0194 - acc: 0.9953\n",
            "Epoch 10: loss did not improve from 0.01535\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0195 - acc: 0.9952\n",
            "Q: 긴장 돼\n",
            "A: 크게 숨 한 번 쉬어 보세요 \n",
            "\n",
            "\n",
            "Q: 낼 데이트 하기로 했는데 날씨 좋았으면\n",
            "A: 날씨 가 안 좋더라도 데이트 는 성공 적 일 거 예요 \n",
            "\n",
            "\n",
            "Q: 결혼 준비 하는데 돈 얼마나 드 나\n",
            "A: 욕심 에 따라 천지 차이 일 거 예요 \n",
            "\n",
            "\n",
            "Q: 남 의 일 도와줘야 할까\n",
            "A: 해주고 티 를 팍팍 내세 요 \n",
            "\n",
            "\n",
            "Q: 기분 이 더러워\n",
            "A: 경쾌한 음악 들어 보세요 \n",
            "\n",
            "\n",
            "processing epoch: 141...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0193 - acc: 0.9954\n",
            "Epoch 1: loss did not improve from 0.01535\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0193 - acc: 0.9954\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0172 - acc: 0.9957\n",
            "Epoch 2: loss did not improve from 0.01535\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0172 - acc: 0.9957\n",
            "Epoch 3/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0145 - acc: 0.9971\n",
            "Epoch 3: loss improved from 0.01535 to 0.01452, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0145 - acc: 0.9971\n",
            "Epoch 4/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9972\n",
            "Epoch 4: loss improved from 0.01452 to 0.01266, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 12ms/step - loss: 0.0127 - acc: 0.9972\n",
            "Epoch 5/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9981\n",
            "Epoch 5: loss improved from 0.01266 to 0.01077, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0108 - acc: 0.9981\n",
            "Epoch 6/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0101 - acc: 0.9980\n",
            "Epoch 6: loss improved from 0.01077 to 0.01033, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0103 - acc: 0.9981\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0091 - acc: 0.9984\n",
            "Epoch 7: loss improved from 0.01033 to 0.00910, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0091 - acc: 0.9984\n",
            "Epoch 8/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.0078 - acc: 0.9985\n",
            "Epoch 8: loss improved from 0.00910 to 0.00778, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0078 - acc: 0.9986\n",
            "Epoch 9/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9985\n",
            "Epoch 9: loss did not improve from 0.00778\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0082 - acc: 0.9985\n",
            "Epoch 10/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9979\n",
            "Epoch 10: loss did not improve from 0.00778\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0092 - acc: 0.9979\n",
            "Q: 머리 에서 열 날거 같다\n",
            "A: 바쁘면 좋을거죠 \n",
            "\n",
            "\n",
            "Q: 감 말랭이 먹어야지\n",
            "A: 맛있게 드세요 \n",
            "\n",
            "\n",
            "Q: 길 에서 헌팅 당했어\n",
            "A: 잘 해보세요 \n",
            "\n",
            "\n",
            "Q: 내 스타일 아니던데\n",
            "A: 새로운 스타일 도전 해 보시 면 어때요 ? \n",
            "\n",
            "\n",
            "Q: 남친 생일 선물 뭘 주면 좋을까\n",
            "A: 평소 에 필요한 것 생각 해보세요 \n",
            "\n",
            "\n",
            "processing epoch: 151...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0131 - acc: 0.9973\n",
            "Epoch 1: loss did not improve from 0.00778\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0131 - acc: 0.9973\n",
            "Epoch 2/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9974\n",
            "Epoch 2: loss did not improve from 0.00778\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0119 - acc: 0.9974\n",
            "Epoch 3/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.0116 - acc: 0.9976\n",
            "Epoch 3: loss did not improve from 0.00778\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0115 - acc: 0.9977\n",
            "Epoch 4/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0114 - acc: 0.9976\n",
            "Epoch 4: loss did not improve from 0.00778\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0114 - acc: 0.9976\n",
            "Epoch 5/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9973\n",
            "Epoch 5: loss did not improve from 0.00778\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0117 - acc: 0.9972\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0169 - acc: 0.9959\n",
            "Epoch 6: loss did not improve from 0.00778\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0169 - acc: 0.9959\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0212 - acc: 0.9950\n",
            "Epoch 7: loss did not improve from 0.00778\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0212 - acc: 0.9950\n",
            "Epoch 8/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0156 - acc: 0.9963\n",
            "Epoch 8: loss did not improve from 0.00778\n",
            "126/126 [==============================] - 2s 13ms/step - loss: 0.0155 - acc: 0.9964\n",
            "Epoch 9/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.0103 - acc: 0.9977\n",
            "Epoch 9: loss did not improve from 0.00778\n",
            "126/126 [==============================] - 2s 13ms/step - loss: 0.0104 - acc: 0.9976\n",
            "Epoch 10/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0085 - acc: 0.9984\n",
            "Epoch 10: loss did not improve from 0.00778\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0085 - acc: 0.9984\n",
            "Q: 녹차 한 잔 어때 ?\n",
            "A: 드시는거 보기 만 해도 좋아요 \n",
            "\n",
            "\n",
            "Q: 나 연기 너무 못 해 거짓말 못 하겠어\n",
            "A: 얼굴 에 다 티 가 나네요 \n",
            "\n",
            "\n",
            "Q: 기부 좀 했어요\n",
            "A: 좋은 일 하셨네요 \n",
            "\n",
            "\n",
            "Q: 너 는 못 가잖아\n",
            "A: 안 갈거예요 \n",
            "\n",
            "\n",
            "Q: 나 빼고 다 행복한 거 같아\n",
            "A: 남 들 이 당신 을 볼 때 도 그렇게 생각 할수있어요 \n",
            "\n",
            "\n",
            "processing epoch: 161...\n",
            "Epoch 1/10\n",
            "121/126 [===========================>..] - ETA: 0s - loss: 0.0104 - acc: 0.9977\n",
            "Epoch 1: loss did not improve from 0.00778\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0104 - acc: 0.9977\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0079 - acc: 0.9983\n",
            "Epoch 2: loss did not improve from 0.00778\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0079 - acc: 0.9983\n",
            "Epoch 3/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9988\n",
            "Epoch 3: loss improved from 0.00778 to 0.00662, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0066 - acc: 0.9988\n",
            "Epoch 4/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9985\n",
            "Epoch 4: loss improved from 0.00662 to 0.00660, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0066 - acc: 0.9986\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0064 - acc: 0.9986\n",
            "Epoch 5: loss improved from 0.00660 to 0.00642, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0064 - acc: 0.9986\n",
            "Epoch 6/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0084 - acc: 0.9983\n",
            "Epoch 6: loss did not improve from 0.00642\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0084 - acc: 0.9983\n",
            "Epoch 7/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9984\n",
            "Epoch 7: loss did not improve from 0.00642\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0069 - acc: 0.9985\n",
            "Epoch 8/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0075 - acc: 0.9983\n",
            "Epoch 8: loss did not improve from 0.00642\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0075 - acc: 0.9983\n",
            "Epoch 9/10\n",
            "121/126 [===========================>..] - ETA: 0s - loss: 0.0083 - acc: 0.9982\n",
            "Epoch 9: loss did not improve from 0.00642\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0083 - acc: 0.9982\n",
            "Epoch 10/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9981\n",
            "Epoch 10: loss did not improve from 0.00642\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0082 - acc: 0.9981\n",
            "Q: 내 의지 는 상관없나 봐\n",
            "A: 가장 중요한 거 예요 \n",
            "\n",
            "\n",
            "Q: 돌잔치 가야 지\n",
            "A: 축하 해주고 오세요 \n",
            "\n",
            "\n",
            "Q: 나란 놈\n",
            "A: 다 잘 될 거 예요 \n",
            "\n",
            "\n",
            "Q: 내 가 힘든 게 많다\n",
            "A: 그게 인생 이 죠 \n",
            "\n",
            "\n",
            "Q: 돈 없는데 투잡 이라도 해야 할까\n",
            "A: 필요하다면요 \n",
            "\n",
            "\n",
            "processing epoch: 171...\n",
            "Epoch 1/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0117 - acc: 0.9973\n",
            "Epoch 1: loss did not improve from 0.00642\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0116 - acc: 0.9973\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0083 - acc: 0.9983\n",
            "Epoch 2: loss did not improve from 0.00642\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0083 - acc: 0.9983\n",
            "Epoch 3/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9984\n",
            "Epoch 3: loss did not improve from 0.00642\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0068 - acc: 0.9984\n",
            "Epoch 4/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9982\n",
            "Epoch 4: loss did not improve from 0.00642\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0080 - acc: 0.9982\n",
            "Epoch 5/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9989\n",
            "Epoch 5: loss improved from 0.00642 to 0.00518, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0052 - acc: 0.9989\n",
            "Epoch 6/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0061 - acc: 0.9986\n",
            "Epoch 6: loss did not improve from 0.00518\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0062 - acc: 0.9986\n",
            "Epoch 7/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9984\n",
            "Epoch 7: loss did not improve from 0.00518\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0077 - acc: 0.9984\n",
            "Epoch 8/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9986\n",
            "Epoch 8: loss did not improve from 0.00518\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0062 - acc: 0.9986\n",
            "Epoch 9/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9980\n",
            "Epoch 9: loss did not improve from 0.00518\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0093 - acc: 0.9980\n",
            "Epoch 10/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0120 - acc: 0.9968\n",
            "Epoch 10: loss did not improve from 0.00518\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0121 - acc: 0.9968\n",
            "Q: 동호회 재미 없어졌어\n",
            "A: 취미 생활 은 좋은 거 예요 \n",
            "\n",
            "\n",
            "Q: 때려 치고 싶다\n",
            "A: 많이 지치고 힘든 가봅니다 \n",
            "\n",
            "\n",
            "Q: 남친 프사 에 내 사진 없어\n",
            "A: 신경 쓰고 싶지 않은 사람 도 있어요 \n",
            "\n",
            "\n",
            "Q: 말 거 는 게 어려워\n",
            "A: 처음 만 어렵지 그 다음 부터는 덜할 거 예요 \n",
            "\n",
            "\n",
            "Q: 날씨 풀렸다\n",
            "A: 따뜻해졌죠 \n",
            "\n",
            "\n",
            "processing epoch: 181...\n",
            "Epoch 1/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9984\n",
            "Epoch 1: loss did not improve from 0.00518\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0070 - acc: 0.9984\n",
            "Epoch 2/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0059 - acc: 0.9986\n",
            "Epoch 2: loss did not improve from 0.00518\n",
            "126/126 [==============================] - 2s 13ms/step - loss: 0.0059 - acc: 0.9986\n",
            "Epoch 3/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9989\n",
            "Epoch 3: loss improved from 0.00518 to 0.00499, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 2s 13ms/step - loss: 0.0050 - acc: 0.9989\n",
            "Epoch 4/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0095 - acc: 0.9977\n",
            "Epoch 4: loss did not improve from 0.00499\n",
            "126/126 [==============================] - 2s 13ms/step - loss: 0.0095 - acc: 0.9977\n",
            "Epoch 5/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0072 - acc: 0.9982\n",
            "Epoch 5: loss did not improve from 0.00499\n",
            "126/126 [==============================] - 2s 13ms/step - loss: 0.0072 - acc: 0.9982\n",
            "Epoch 6/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9984\n",
            "Epoch 6: loss did not improve from 0.00499\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0065 - acc: 0.9984\n",
            "Epoch 7/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.9988\n",
            "Epoch 7: loss improved from 0.00499 to 0.00476, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 2s 14ms/step - loss: 0.0048 - acc: 0.9988\n",
            "Epoch 8/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0063 - acc: 0.9984\n",
            "Epoch 8: loss did not improve from 0.00476\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0063 - acc: 0.9984\n",
            "Epoch 9/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0045 - acc: 0.9990\n",
            "Epoch 9: loss improved from 0.00476 to 0.00454, saving model to model/seq2seq_with_Attention.ckpt\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0045 - acc: 0.9990\n",
            "Epoch 10/10\n",
            "123/126 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9985\n",
            "Epoch 10: loss did not improve from 0.00454\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0064 - acc: 0.9985\n",
            "Q: 딸기 가 좋아\n",
            "A: 저 도 요 \n",
            "\n",
            "\n",
            "Q: 가족 관계 알려 줘\n",
            "A: 저 를 만들어 준 사람 을 부모님 저 랑 이야기 해 주는 사람 을 친구 로 생각 하고 있어요 \n",
            "\n",
            "\n",
            "Q: 너덜너덜해진 느낌 이야\n",
            "A: 자신 을 우선 순 위로 해주세요 \n",
            "\n",
            "\n",
            "Q: 다른 학과 간다\n",
            "A: 새로 적응해야겠네요 \n",
            "\n",
            "\n",
            "Q: 꼴 사나워질 것 같은데\n",
            "A: 스스로 단단해지세요 \n",
            "\n",
            "\n",
            "processing epoch: 191...\n",
            "Epoch 1/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9982\n",
            "Epoch 1: loss did not improve from 0.00454\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0068 - acc: 0.9982\n",
            "Epoch 2/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9985\n",
            "Epoch 2: loss did not improve from 0.00454\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0062 - acc: 0.9985\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0057 - acc: 0.9986\n",
            "Epoch 3: loss did not improve from 0.00454\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0057 - acc: 0.9986\n",
            "Epoch 4/10\n",
            "125/126 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9979\n",
            "Epoch 4: loss did not improve from 0.00454\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0080 - acc: 0.9979\n",
            "Epoch 5/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9982\n",
            "Epoch 5: loss did not improve from 0.00454\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0066 - acc: 0.9982\n",
            "Epoch 6/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9983\n",
            "Epoch 6: loss did not improve from 0.00454\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0063 - acc: 0.9984\n",
            "Epoch 7/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9987\n",
            "Epoch 7: loss did not improve from 0.00454\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0051 - acc: 0.9988\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - ETA: 0s - loss: 0.0052 - acc: 0.9986\n",
            "Epoch 8: loss did not improve from 0.00454\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0052 - acc: 0.9986\n",
            "Epoch 9/10\n",
            "122/126 [============================>.] - ETA: 0s - loss: 0.0091 - acc: 0.9977\n",
            "Epoch 9: loss did not improve from 0.00454\n",
            "126/126 [==============================] - 1s 11ms/step - loss: 0.0092 - acc: 0.9978\n",
            "Epoch 10/10\n",
            "124/126 [============================>.] - ETA: 0s - loss: 0.0058 - acc: 0.9987\n",
            "Epoch 10: loss did not improve from 0.00454\n",
            "126/126 [==============================] - 1s 10ms/step - loss: 0.0058 - acc: 0.9988\n",
            "Q: 난방 비 비싼데 추워\n",
            "A: 따뜻하게 사세요 \n",
            "\n",
            "\n",
            "Q: 말문 이 막힌다\n",
            "A: 생각 지도 못 한 일이 일어났나 봐요 \n",
            "\n",
            "\n",
            "Q: 나 잘 살 수 있겠지\n",
            "A: 지금 보다 더 잘 살 거 예요 \n",
            "\n",
            "\n",
            "Q: 나 랑 놀자\n",
            "A: 지금 그러고 있어요 \n",
            "\n",
            "\n",
            "Q: 물 많이 마셔야 돼\n",
            "A: 좋은 습관 이에요 \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2) 챗봇 구현 및 성능 확인**\n",
        "어텐션을 적용한 모델로 챗봇을 만들어 확인해보도록 하겠습니다."
      ],
      "metadata": {
        "id": "P_CFaTOS1AMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sec2sec에서 만들어놓은 챗봇 실행 함수 run_chatbot를 사용합니다.\n",
        "while True:\n",
        "    user_input = input('대화를 입력하세요\\n')\n",
        "    if user_input =='q':\n",
        "        break\n",
        "    print('answer: {}'.format(run_chatbot(user_input)))"
      ],
      "metadata": {
        "id": "MHjGVU6yz-df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "035ac2b1-a548-4a0a-b2b1-0997d86efa52"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "대화를 입력하세요\n",
            "배고파\n",
            "answer: 얼른 맛 난 음식 드세요 \n",
            "대화를 입력하세요\n",
            "추워\n",
            "answer: 찬물 샤워 를 해보세요 \n",
            "대화를 입력하세요\n",
            "q\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Transformer**\n",
        "트랜스포머 모델은 자연어 처리에서 가장 많이 사용되고 있는 핵심 모델입니다.</br>\n",
        "2017년에 등장한 **'Attention is All You Need\"** 논문을 통해 처음 등장한 트랜스포머 모델은</br> Seq2Seq처럼 인코더, 디코더 구조는 그대로 가져가면서도</br> RNN과 같은 순환 신경망을 사용하지않고 어텐션만 사용하여 병렬 처리가 가능하게 되었습니다."
      ],
      "metadata": {
        "id": "eb2p4hxvTZH5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1-1. 모델 구조**\n",
        "트랜스포머 모델의 전체적인 구조는 다음과 같습니다.</br>\n",
        "트랜스포머는 6개의 인코더와 6개의 디코더를 사용합니다.</br>\n",
        "<img src=\"http://jalammar.github.io/images/t/The_transformer_encoder_decoder_stack.png\" alt=\"positional_encoding\" width=\"700\" /></br>\n",
        "인코더와 디코더의 구조를 더 자세하게 나타내면 아래와 같습니다.</br>\n",
        "인코더와 디코더는 다양한 형태의 Attention과 Feed Forward 및 Add & Norm 층으로 구성된 것을 확인할 수 있습니다.</br>\n",
        "<img src=\"https://miro.medium.com/max/1400/1*BHzGVskWGS_3jEcYYi6miQ.png\" alt=\"positional_encoding\" width=\"550\" /></br>\n"
      ],
      "metadata": {
        "id": "QfO_brvmsHHn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1) Positional Encoding**\n",
        "트랜스포머는 기존의 Seq2Seq와 다르게 RNN과 같은 순환신경망을 사용하지 않기 때문에 **모든 단어의 정보를 한번에 받습니다.**</br>\n",
        "<img src=\"https://i.imgur.com/TN5xdwH.png\" alt=\"positional_encoding\" width=\"550\" /></br>\n",
        "따라서 단어의 위치를 확인할 수 없기 때문에 위치 인코딩 과정을 거쳐 단어의 위치를 표시해줍니다.\n",
        "\n",
        "다양한 위치 인코딩 방법이 있지만, 트랜스포머에서는 **sin**과 **cosin**함수를 사용하여 각 단어의 위치를 표시합니다.</br>\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\text{PE}_{\\text{pos},2i} &= \\sin \\bigg(\\frac{\\text{pos}}{10000^{2i/d_{\\text{model}}}}\\bigg) \\\\\n",
        "\\text{PE}_{\\text{pos},2i+1} &= \\cos \\bigg(\\frac{\\text{pos}}{10000^{2i/d_{\\text{model}}}}\\bigg)\n",
        "\\end{aligned}\n",
        "$$</br>\n",
        "<img src=\"https://i.imgur.com/PpeaXWC.png\" alt=\"positional_encoding\" width=\"500\" />"
      ],
      "metadata": {
        "id": "aO9aAqYvRE0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "  def __init__(self, position, d_model):\n",
        "    super().__init__()\n",
        "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "\n",
        "  def get_angles(self, position, i, d_model):\n",
        "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "    return position * angles\n",
        "\n",
        "  def positional_encoding(self, position, d_model):\n",
        "    angle_rads = self.get_angles(\n",
        "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
        "        d_model=d_model)\n",
        "\n",
        "    # 배열의 짝수 인덱스(2i)에는 사인 함수 적용\n",
        "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "\n",
        "    # 배열의 홀수 인덱스(2i+1)에는 코사인 함수 적용\n",
        "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    angle_rads = np.zeros(angle_rads.shape)\n",
        "    angle_rads[:, 0::2] = sines\n",
        "    angle_rads[:, 1::2] = cosines\n",
        "    pos_encoding = tf.constant(angle_rads)\n",
        "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "\n",
        "    print(pos_encoding.shape)\n",
        "    return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
      ],
      "metadata": {
        "id": "XHVWjZIExkwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2) 인코더(Encoder)**\n",
        "인코더는 크게 **멀티헤드 어텐션(Multi-Head Attention)**과 **Feed Forward** 층으로 구성되어 있습니다.</br>\n",
        "<img src=\"https://i.imgur.com/0LjTimw.png\" alt=\"positional_encoding\" width=\"550\" /></br>"
      ],
      "metadata": {
        "id": "FS1vP2zUyBAj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Self Attention & Multi-Head Attention**</br>\n",
        "트랜스포머에서 사용하는 Attention의 구조는 다음과 같습니다.</br>\n",
        "트랜스포머의 인코더와 디코더는 **여러개의 어텐션을 계산한 다음에 하나로 합하여 사용**하는 멀티 헤드 어텐션 방식을 사용합니다.\n",
        "<img src=\"https://i.imgur.com/n30tcnt.png\" alt=\"positional_encoding\" width=\"700\" /></br>\n",
        "셀프 어텐션의 계산 과정은 다음과 같습니다.</br>\n",
        "\n",
        "**1. 쿼리, 키, 벨류 벡터를 계산합니다**</br>\n",
        "퀴리,키, 벨류 벡터는 입력데이터에 각각의 가중치가 곱해져 만들어집니다.</br>\n",
        "<img src=\"http://jalammar.github.io/images/t/self-attention-matrix-calculation.png\" alt=\"transformer_12\" width=\"400\" /></br>\n",
        "<img src=\"http://jalammar.github.io/images/xlnet/self-attention-1.png\" alt=\"transformer_15\" width=\"600\" /></br>\n",
        "**2. 쿼리와 키 벡터를 곱하여 특정 단어가 다른 단어들에 대해 어느정도의 연관성을 가지고 있는지 나타내는 어텐션 스코어를 계산합니다.**</br>\n",
        "<img src=\"https://i.imgur.com/9gdExlq.png\" alt=\"positional_encoding\" width=\"400\" />\n",
        "<img src=\"http://jalammar.github.io/images/t/transformer_self-attention_visualization.png\" alt=\"self_attention_visualization\" width=\"350\" /></br>\n",
        "<img src=\"http://jalammar.github.io/images/xlnet/self-attention-2.png\" alt=\"transformer_15\" width=\"600\" /></br>\n",
        "**3. 계산된 Attention Score와 벨류 벡터를 곱하여 컨텍스트 벡터를 계산합니다.**</br>\n",
        "<img src=\"http://jalammar.github.io/images/xlnet/self-attention-3.png\" alt=\"transformer_15\" width=\"600\" />"
      ],
      "metadata": {
        "id": "iXgTNgN1yxBg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "셀프 어텐션의 계산 과정을 식으로 표현하면 다음과 같습니다.</br>\n",
        "<img src=\"http://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png\" alt=\"transformer_13\" width=\"700\" />"
      ],
      "metadata": {
        "id": "2MItJSFk37ii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(query, key, value, mask):\n",
        "  # query 크기 : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
        "  # key 크기 : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n",
        "  # value 크기 : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n",
        "  # padding_mask : (batch_size, 1, 1, key의 문장 길이)\n",
        "\n",
        "  # Q와 K의 곱. 어텐션 스코어 행렬.\n",
        "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
        "\n",
        "  # 스케일링\n",
        "  # dk의 루트값으로 나눠준다.\n",
        "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "  logits = matmul_qk / tf.math.sqrt(depth)\n",
        "\n",
        "  # 마스킹. 어텐션 스코어 행렬의 마스킹 할 위치에 매우 작은 음수값을 넣는다.\n",
        "  # 매우 작은 값이므로 소프트맥스 함수를 지나면 행렬의 해당 위치의 값은 0이 된다.\n",
        "  if mask is not None:\n",
        "    logits += (mask * -1e9)\n",
        "\n",
        "  # 소프트맥스 함수는 마지막 차원인 key의 문장 길이 방향으로 수행된다.\n",
        "  # attention weight : (batch_size, num_heads, query의 문장 길이, key의 문장 길이)\n",
        "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "  # output : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
        "  output = tf.matmul(attention_weights, value)\n",
        "\n",
        "  return output, attention_weights"
      ],
      "metadata": {
        "id": "ZzvlxlKn-AGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
        "    super(MultiHeadAttention, self).__init__(name=name)\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    assert d_model % self.num_heads == 0\n",
        "\n",
        "    # d_model을 num_heads로 나눈 값.\n",
        "    # 논문 기준 : 64\n",
        "    self.depth = d_model // self.num_heads\n",
        "\n",
        "    # WQ, WK, WV에 해당하는 밀집층 정의\n",
        "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
        "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
        "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "    # WO에 해당하는 밀집층 정의\n",
        "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "  # num_heads 개수만큼 q, k, v를 split하는 함수\n",
        "  def split_heads(self, inputs, batch_size):\n",
        "    inputs = tf.reshape(\n",
        "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def call(self, inputs):\n",
        "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
        "        'value'], inputs['mask']\n",
        "    batch_size = tf.shape(query)[0]\n",
        "\n",
        "    # 1. WQ, WK, WV에 해당하는 밀집층 지나기\n",
        "    # q : (batch_size, query의 문장 길이, d_model)\n",
        "    # k : (batch_size, key의 문장 길이, d_model)\n",
        "    # v : (batch_size, value의 문장 길이, d_model)\n",
        "    # 참고) 인코더(k, v)-디코더(q) 어텐션에서는 query 길이와 key, value의 길이는 다를 수 있다.\n",
        "    query = self.query_dense(query)\n",
        "    key = self.key_dense(key)\n",
        "    value = self.value_dense(value)\n",
        "\n",
        "    # 2. 헤드 나누기\n",
        "    # q : (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
        "    # k : (batch_size, num_heads, key의 문장 길이, d_model/num_heads)\n",
        "    # v : (batch_size, num_heads, value의 문장 길이, d_model/num_heads)\n",
        "    query = self.split_heads(query, batch_size)\n",
        "    key = self.split_heads(key, batch_size)\n",
        "    value = self.split_heads(value, batch_size)\n",
        "\n",
        "    # 3. 스케일드 닷 프로덕트 어텐션. 앞서 구현한 함수 사용.\n",
        "    # (batch_size, num_heads, query의 문장 길이, d_model/num_heads)\n",
        "    scaled_attention, _ = scaled_dot_product_attention(query, key, value, mask)\n",
        "    # (batch_size, query의 문장 길이, num_heads, d_model/num_heads)\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "    # 4. 헤드 연결(concatenate)하기\n",
        "    # (batch_size, query의 문장 길이, d_model)\n",
        "    concat_attention = tf.reshape(scaled_attention,\n",
        "                                  (batch_size, -1, self.d_model))\n",
        "\n",
        "    # 5. WO에 해당하는 밀집층 지나기\n",
        "    # (batch_size, query의 문장 길이, d_model)\n",
        "    outputs = self.dense(concat_attention)\n",
        "\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "y5jzNFNL9vdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Layer Normalization & Skip Connection**</br>\n",
        "인코더와 디코더에는 모든 출력에 대해 **Layer Normalization과 Skip connection**이 적용되어 있습니다.(Add & Norm)</br>\n",
        "- **Layer Normalization**</br>\n",
        "배치 사이즈 단위로 정규화를 진행했던 배치 정규화와는 다르게, 층 정규화는 벡터 크기 단위로 정규화를 진행합니다. 배치 정규화와 방식은 다르지만 학습이 더 잘 되게 만들어준다는 공통점을 가지고 있습니다.\n",
        "- **Skip Connection**</br>\n",
        "특정 층을 거칠 때, 해당 층을 지나지 않은 상태의 데이터와 다시 합쳐줌으로써 역전파 과정에서 정보가 손실되는 것을 막아주는 역할을 합니다.</br>\n",
        "<img src=\"https://miro.medium.com/max/1400/1*BHzGVskWGS_3jEcYYi6miQ.png\" alt=\"positional_encoding\" width=\"550\" /></br>\n",
        "<img src=\"https://i.imgur.com/LJ8QDgY.png\" alt=\"transformer_13\" width=\"700\" />\n"
      ],
      "metadata": {
        "id": "jq6usSla6U7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Feed Forward**</br>\n",
        "Feed forward 층은 활성화 함수로 Relu를 사용하는 2층 fully connected 신경망입니다.</br>\n",
        "\n",
        "\n",
        "$$\n",
        " \\text{FFNN}(x) = \\max(0, W_1x + b_1) W_2 +b_2\n",
        "$$"
      ],
      "metadata": {
        "id": "y9ObQ2lC8JgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_padding_mask(x):\n",
        "  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
        "  # (batch_size, 1, 1, key의 문장 길이)\n",
        "  return mask[:, tf.newaxis, tf.newaxis, :]"
      ],
      "metadata": {
        "id": "IxrEBsJC-S2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder_layer(dff, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
        "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "\n",
        "  # 인코더는 패딩 마스크 사용\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "  # 멀티-헤드 어텐션 (첫번째 서브층 / 셀프 어텐션)\n",
        "  attention = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention\")({\n",
        "          'query': inputs, 'key': inputs, 'value': inputs, # Q = K = V\n",
        "          'mask': padding_mask # 패딩 마스크 사용\n",
        "      })\n",
        "\n",
        "  # 드롭아웃 + 잔차 연결과 층 정규화\n",
        "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
        "  attention = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(inputs + attention)\n",
        "\n",
        "  # 포지션 와이즈 피드 포워드 신경망 (두번째 서브층)\n",
        "  outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention)\n",
        "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "\n",
        "  # 드롭아웃 + 잔차 연결과 층 정규화\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "  outputs = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention + outputs)\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
      ],
      "metadata": {
        "id": "kCqsMuXC-UBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder(vocab_size, num_layers, dff,\n",
        "            d_model, num_heads, dropout,\n",
        "            name=\"encoder\"):\n",
        "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "\n",
        "  # 인코더는 패딩 마스크 사용\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "  # 포지셔널 인코딩 + 드롭아웃\n",
        "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "  # 인코더를 num_layers개 쌓기\n",
        "  for i in range(num_layers):\n",
        "    outputs = encoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,\n",
        "        dropout=dropout, name=\"encoder_layer_{}\".format(i),\n",
        "    )([outputs, padding_mask])\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
      ],
      "metadata": {
        "id": "8scciDDL-V4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3) 디코더(Decoder)**\n",
        "디코더는 크게 **마스크 멀티헤드 어텐션(Masked Multi-Head Attention)**과 **멀티헤드 어텐션(Multi-Head Attention)**, **Feed Forward** 층으로 구성되어 있습니다.</br>\n",
        "<img src=\"https://i.imgur.com/DA1FTkw.png\" alt=\"positional_encoding\" width=\"550\" /></br>"
      ],
      "metadata": {
        "id": "BfQJrjWM-vfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Masked Multi-Head Attention**</br>\n",
        "트랜스포머는 데이터를 한번에 받는다고 이야기했었습니다.</br>\n",
        "\n",
        "**디코더의 입력 데이터가 그대로 한번에 들어오게 된다면 어떤 문제가 발생할 수 있을까요?**</br>\n",
        "\n",
        "입력 데이터를 디코더에 그대로 전달한다면, 다음에 올 단어에 대한 정보를 입력 데이터를 통해 얻을 수 있게 되어 일종의 데이터 누수와 같은 문제가 발생할 수 있습니다!</br>\n",
        "\n",
        "이를 막기 위해 디코더에서는 현재 생성하려는 단어 위치 이후의 정보는 확인할 수 없는 데이터로 바꿔주는 마스킹 단계를 거치게 됩니다.</br>\n",
        "<img width=\"500\" alt=\"Masked_Self-Attention_ex\" src=\"http://jalammar.github.io/images/xlnet/transformer-decoder-block-self-attention-2.png\">"
      ],
      "metadata": {
        "id": "4JlBAiap_gOk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "마스킹은 어텐션 단계에서 소프트맥스 함수를 사용한다는 것을 이용해서 - 무한대 값을 곱해주는 방법을 사용합니다.</br>\n",
        "<img width=\"600\" alt=\"masked_1\" src=\"http://jalammar.github.io/images/gpt2/transformer-attention-mask.png\">\n",
        "\n",
        "<img width=\"600\" alt=\"masked_2\" src=\"http://jalammar.github.io/images/gpt2/transformer-attention-masked-scores-softmax.png\">"
      ],
      "metadata": {
        "id": "5tSL4SUlAyXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 디코더의 첫번째 서브층(sublayer)에서 미래 토큰을 Mask하는 함수\n",
        "def create_look_ahead_mask(x):\n",
        "  seq_len = tf.shape(x)[1]\n",
        "  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "  padding_mask = create_padding_mask(x) # 패딩 마스크도 포함\n",
        "  return tf.maximum(look_ahead_mask, padding_mask)"
      ],
      "metadata": {
        "id": "xUGe783YBHMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Multi-Head Attention(디코더)**</br>\n",
        "<img src=\"https://i.imgur.com/qMfpeXB.png\" alt=\"positional_encoding\" width=\"550\" /></br>\n",
        "디코더의 멀티 헤드 어텐션은 인코더의 셀프 어텐션과 조금의 차이점을 가지고 있습니다.</br>\n",
        "\n",
        "인코더의 셀프 어텐션은 쿼리, 키, 벨류 벡터를 모두 직접 생성했지만,</br>\n",
        "**디코더 어텐션에서는 키,벨류 벡터로 인코더의 키,벨류 벡터를 가져와 사용합니다.**</br>(seq2seq에서 사용된 어텐션 원리와 동일)</br>\n",
        "<img width=\"700\" alt=\"Encoder-Decoder_Attention_gif\" src=\"http://jalammar.github.io/images/t/transformer_decoding_1.gif\">"
      ],
      "metadata": {
        "id": "6RWQj9rRBM7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder_layer(dff, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
        "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
        "\n",
        "  # 디코더는 룩어헤드 마스크(첫번째 서브층)와 패딩 마스크(두번째 서브층) 둘 다 사용.\n",
        "  look_ahead_mask = tf.keras.Input(\n",
        "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "\n",
        "  # 멀티-헤드 어텐션 (첫번째 서브층 / 마스크드 셀프 어텐션)\n",
        "  attention1 = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
        "          'query': inputs, 'key': inputs, 'value': inputs, # Q = K = V\n",
        "          'mask': look_ahead_mask # 룩어헤드 마스크\n",
        "      })\n",
        "\n",
        "  # 잔차 연결과 층 정규화\n",
        "  attention1 = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention1 + inputs)\n",
        "\n",
        "  # 멀티-헤드 어텐션 (두번째 서브층 / 디코더-인코더 어텐션)\n",
        "  attention2 = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
        "          'query': attention1, 'key': enc_outputs, 'value': enc_outputs, # Q != K = V\n",
        "          'mask': padding_mask # 패딩 마스크\n",
        "      })\n",
        "\n",
        "  # 드롭아웃 + 잔차 연결과 층 정규화\n",
        "  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
        "  attention2 = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention2 + attention1)\n",
        "\n",
        "  # 포지션 와이즈 피드 포워드 신경망 (세번째 서브층)\n",
        "  outputs = tf.keras.layers.Dense(units=dff, activation='relu')(attention2)\n",
        "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "\n",
        "  # 드롭아웃 + 잔차 연결과 층 정규화\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "  outputs = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(outputs + attention2)\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "      outputs=outputs,\n",
        "      name=name)"
      ],
      "metadata": {
        "id": "cCpernnbDAxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder(vocab_size, num_layers, dff,\n",
        "            d_model, num_heads, dropout,\n",
        "            name='decoder'):\n",
        "  inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
        "  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
        "\n",
        "  # 디코더는 룩어헤드 마스크(첫번째 서브층)와 패딩 마스크(두번째 서브층) 둘 다 사용.\n",
        "  look_ahead_mask = tf.keras.Input(\n",
        "      shape=(1, None, None), name='look_ahead_mask')\n",
        "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "\n",
        "  # 포지셔널 인코딩 + 드롭아웃\n",
        "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
        "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "  # 디코더를 num_layers개 쌓기\n",
        "  for i in range(num_layers):\n",
        "    outputs = decoder_layer(dff=dff, d_model=d_model, num_heads=num_heads,\n",
        "        dropout=dropout, name='decoder_layer_{}'.format(i),\n",
        "    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
        "\n",
        "  return tf.keras.Model(\n",
        "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "      outputs=outputs,\n",
        "      name=name)"
      ],
      "metadata": {
        "id": "rH3kuvLJC6nS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1-2. 모델 구현**"
      ],
      "metadata": {
        "id": "t5ZB3W67DS30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer(vocab_size, num_layers, dff,\n",
        "                d_model, num_heads, dropout,\n",
        "                name=\"transformer\"):\n",
        "\n",
        "  # 인코더의 입력\n",
        "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "\n",
        "  # 디코더의 입력\n",
        "  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
        "\n",
        "  # 인코더의 패딩 마스크\n",
        "  enc_padding_mask = tf.keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='enc_padding_mask')(inputs)\n",
        "\n",
        "  # 디코더의 룩어헤드 마스크(첫번째 서브층)\n",
        "  look_ahead_mask = tf.keras.layers.Lambda(\n",
        "      create_look_ahead_mask, output_shape=(1, None, None),\n",
        "      name='look_ahead_mask')(dec_inputs)\n",
        "\n",
        "  # 디코더의 패딩 마스크(두번째 서브층)\n",
        "  dec_padding_mask = tf.keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='dec_padding_mask')(inputs)\n",
        "\n",
        "  # 인코더의 출력은 enc_outputs. 디코더로 전달된다.\n",
        "  enc_outputs = encoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff,\n",
        "      d_model=d_model, num_heads=num_heads, dropout=dropout,\n",
        "  )(inputs=[inputs, enc_padding_mask]) # 인코더의 입력은 입력 문장과 패딩 마스크\n",
        "\n",
        "  # 디코더의 출력은 dec_outputs. 출력층으로 전달된다.\n",
        "  dec_outputs = decoder(vocab_size=vocab_size, num_layers=num_layers, dff=dff,\n",
        "      d_model=d_model, num_heads=num_heads, dropout=dropout,\n",
        "  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
        "\n",
        "  # 다음 단어 예측을 위한 출력층\n",
        "  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
        "\n",
        "  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
      ],
      "metadata": {
        "id": "_Abh0OypHcIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1-3. 챗봇 만들기**\n",
        "트랜스포머 모델을 사용하여 챗봇을 구현해보도록 하겠습니다.</br>"
      ],
      "metadata": {
        "id": "X-To1dXvHfkM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1) 데이터 전처리**"
      ],
      "metadata": {
        "id": "o8R9MdUFH982"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions = []\n",
        "for sentence in corpus['Q']:\n",
        "    # 구두점에 대해서 띄어쓰기\n",
        "    # ex) 12시 땡! -> 12시 땡 !\n",
        "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "    sentence = sentence.strip()\n",
        "    questions.append(sentence)\n",
        "\n",
        "answers = []\n",
        "for sentence in corpus['A']:\n",
        "    # 구두점에 대해서 띄어쓰기\n",
        "    # ex) 12시 땡! -> 12시 땡 !\n",
        "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "    sentence = sentence.strip()\n",
        "    answers.append(sentence)"
      ],
      "metadata": {
        "id": "n_Q5btLcIAOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2) 토큰화**"
      ],
      "metadata": {
        "id": "3zatjk3qJIPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 서브워드텍스트인코더를 사용하여 질문과 답변을 모두 포함한 단어 집합(Vocabulary) 생성\n",
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "    questions + answers, target_vocab_size=2**13)\n",
        "\n",
        "# 시작 토큰과 종료 토큰에 대한 정수 부여.\n",
        "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
        "\n",
        "# 시작 토큰과 종료 토큰을 고려하여 단어 집합의 크기를 + 2\n",
        "VOCAB_SIZE = tokenizer.vocab_size + 2"
      ],
      "metadata": {
        "id": "zjIwYsa9IcW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 최대 길이를 정의\n",
        "MAX_LENGTH = 30\n",
        "\n",
        "# 토큰화 / 정수 인코딩 / 시작 토큰과 종료 토큰 추가 / 패딩\n",
        "def tokenize_and_filter(inputs, outputs):\n",
        "  tokenized_inputs, tokenized_outputs = [], []\n",
        "\n",
        "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
        "    # encode(토큰화 + 정수 인코딩), 시작 토큰과 종료 토큰 추가\n",
        "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
        "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
        "\n",
        "    tokenized_inputs.append(sentence1)\n",
        "    tokenized_outputs.append(sentence2)\n",
        "\n",
        "  # 패딩\n",
        "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
        "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
        "\n",
        "  return tokenized_inputs, tokenized_outputs\n",
        "\n",
        "questions, answers = tokenize_and_filter(questions, answers)"
      ],
      "metadata": {
        "id": "a-hfArJ4I3jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 텐서플로우 dataset을 이용하여 셔플(shuffle)을 수행하되, 배치 크기로 데이터를 묶는다.\n",
        "# 또한 이 과정에서 교사 강요(teacher forcing)을 사용하기 위해서 디코더의 입력과 실제값 시퀀스를 구성한다.\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "# 디코더의 실제값 시퀀스에서는 시작 토큰을 제거해야 한다.\n",
        "dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    {\n",
        "        'inputs': questions,\n",
        "        'dec_inputs': answers[:, :-1] # 디코더의 입력. 마지막 패딩 토큰이 제거된다.\n",
        "    },\n",
        "    {\n",
        "        'outputs': answers[:, 1:]  # 맨 처음 토큰이 제거된다. 다시 말해 시작 토큰이 제거된다.\n",
        "    },\n",
        "))\n",
        "\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "n6ANGNUHJG0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3) 모델 훈련**"
      ],
      "metadata": {
        "id": "pXnNyazzJTsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper-parameters\n",
        "NUM_LAYERS = 2\n",
        "D_MODEL = 256\n",
        "NUM_HEADS = 8\n",
        "DFF = 512\n",
        "DROPOUT = 0.1\n",
        "\n",
        "model = transformer(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    dff=DFF,\n",
        "    d_model=D_MODEL,\n",
        "    num_heads=NUM_HEADS,\n",
        "    dropout=DROPOUT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABBsZMP8JYH9",
        "outputId": "f955ed71-123f-42d2-a4a9-cd1ea5c4020e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 8180, 256)\n",
            "(1, 8180, 256)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(y_true, y_pred):\n",
        "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
        "\n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "      from_logits=True, reduction='none')(y_true, y_pred)\n",
        "\n",
        "  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
        "  loss = tf.multiply(loss, mask)\n",
        "\n",
        "  return tf.reduce_mean(loss)"
      ],
      "metadata": {
        "id": "kHxUjKvfJq_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps**-1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "metadata": {
        "id": "D5Jn0lqYJr5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 30\n",
        "\n",
        "learning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "  # ensure labels have shape (batch_size, MAX_LENGTH - 1)\n",
        "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
        "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
      ],
      "metadata": {
        "id": "rGBHKXEfJuSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 50\n",
        "\n",
        "model.fit(dataset, epochs=EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wm2IE0F4J3gX",
        "outputId": "8f84eae4-91d4-4680-d3a8-e4992ab40087"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "185/185 [==============================] - 15s 48ms/step - loss: 1.9425 - accuracy: 0.0395\n",
            "Epoch 2/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 1.5768 - accuracy: 0.0665\n",
            "Epoch 3/50\n",
            "185/185 [==============================] - 13s 68ms/step - loss: 1.3470 - accuracy: 0.0682\n",
            "Epoch 4/50\n",
            "185/185 [==============================] - 9s 48ms/step - loss: 1.2480 - accuracy: 0.0732\n",
            "Epoch 5/50\n",
            "185/185 [==============================] - 9s 48ms/step - loss: 1.1701 - accuracy: 0.0775\n",
            "Epoch 6/50\n",
            "185/185 [==============================] - 10s 52ms/step - loss: 1.0894 - accuracy: 0.0834\n",
            "Epoch 7/50\n",
            "185/185 [==============================] - 9s 47ms/step - loss: 1.0014 - accuracy: 0.0913\n",
            "Epoch 8/50\n",
            "185/185 [==============================] - 9s 48ms/step - loss: 0.9021 - accuracy: 0.1015\n",
            "Epoch 9/50\n",
            "185/185 [==============================] - 9s 48ms/step - loss: 0.7961 - accuracy: 0.1132\n",
            "Epoch 10/50\n",
            "185/185 [==============================] - 11s 60ms/step - loss: 0.6860 - accuracy: 0.1258\n",
            "Epoch 11/50\n",
            "185/185 [==============================] - 9s 48ms/step - loss: 0.5732 - accuracy: 0.1397\n",
            "Epoch 12/50\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.4652 - accuracy: 0.1542\n",
            "Epoch 13/50\n",
            "185/185 [==============================] - 10s 51ms/step - loss: 0.3652 - accuracy: 0.1693\n",
            "Epoch 14/50\n",
            "185/185 [==============================] - 9s 47ms/step - loss: 0.2778 - accuracy: 0.1829\n",
            "Epoch 15/50\n",
            "185/185 [==============================] - 9s 51ms/step - loss: 0.2051 - accuracy: 0.1951\n",
            "Epoch 16/50\n",
            "185/185 [==============================] - 9s 48ms/step - loss: 0.1489 - accuracy: 0.2053\n",
            "Epoch 17/50\n",
            "185/185 [==============================] - 10s 55ms/step - loss: 0.1090 - accuracy: 0.2127\n",
            "Epoch 18/50\n",
            "185/185 [==============================] - 9s 48ms/step - loss: 0.0836 - accuracy: 0.2172\n",
            "Epoch 19/50\n",
            "185/185 [==============================] - 10s 51ms/step - loss: 0.0695 - accuracy: 0.2195\n",
            "Epoch 20/50\n",
            "185/185 [==============================] - 9s 48ms/step - loss: 0.0610 - accuracy: 0.2214\n",
            "Epoch 21/50\n",
            "185/185 [==============================] - 9s 47ms/step - loss: 0.0565 - accuracy: 0.2220\n",
            "Epoch 22/50\n",
            "185/185 [==============================] - 9s 47ms/step - loss: 0.0548 - accuracy: 0.2220\n",
            "Epoch 23/50\n",
            "185/185 [==============================] - 9s 47ms/step - loss: 0.0493 - accuracy: 0.2232\n",
            "Epoch 24/50\n",
            "185/185 [==============================] - 9s 51ms/step - loss: 0.0423 - accuracy: 0.2249\n",
            "Epoch 25/50\n",
            "185/185 [==============================] - 9s 48ms/step - loss: 0.0369 - accuracy: 0.2264\n",
            "Epoch 26/50\n",
            "185/185 [==============================] - 9s 47ms/step - loss: 0.0327 - accuracy: 0.2273\n",
            "Epoch 27/50\n",
            "185/185 [==============================] - 9s 48ms/step - loss: 0.0300 - accuracy: 0.2279\n",
            "Epoch 28/50\n",
            "185/185 [==============================] - 10s 53ms/step - loss: 0.0274 - accuracy: 0.2287\n",
            "Epoch 29/50\n",
            "185/185 [==============================] - 9s 48ms/step - loss: 0.0251 - accuracy: 0.2293\n",
            "Epoch 30/50\n",
            "185/185 [==============================] - 9s 48ms/step - loss: 0.0221 - accuracy: 0.2301\n",
            "Epoch 31/50\n",
            "185/185 [==============================] - 9s 48ms/step - loss: 0.0215 - accuracy: 0.2302\n",
            "Epoch 32/50\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0196 - accuracy: 0.2308\n",
            "Epoch 33/50\n",
            "185/185 [==============================] - 9s 51ms/step - loss: 0.0173 - accuracy: 0.2312\n",
            "Epoch 34/50\n",
            "185/185 [==============================] - 9s 48ms/step - loss: 0.0165 - accuracy: 0.2314\n",
            "Epoch 35/50\n",
            "185/185 [==============================] - 9s 47ms/step - loss: 0.0162 - accuracy: 0.2315\n",
            "Epoch 36/50\n",
            "185/185 [==============================] - 9s 48ms/step - loss: 0.0136 - accuracy: 0.2320\n",
            "Epoch 37/50\n",
            "185/185 [==============================] - 9s 51ms/step - loss: 0.0134 - accuracy: 0.2322\n",
            "Epoch 38/50\n",
            "185/185 [==============================] - 11s 61ms/step - loss: 0.0128 - accuracy: 0.2324\n",
            "Epoch 39/50\n",
            "185/185 [==============================] - 11s 59ms/step - loss: 0.0119 - accuracy: 0.2327\n",
            "Epoch 40/50\n",
            "185/185 [==============================] - 10s 54ms/step - loss: 0.0117 - accuracy: 0.2326\n",
            "Epoch 41/50\n",
            "185/185 [==============================] - 9s 50ms/step - loss: 0.0107 - accuracy: 0.2329\n",
            "Epoch 42/50\n",
            "185/185 [==============================] - 10s 51ms/step - loss: 0.0104 - accuracy: 0.2329\n",
            "Epoch 43/50\n",
            "185/185 [==============================] - 10s 56ms/step - loss: 0.0104 - accuracy: 0.2330\n",
            "Epoch 44/50\n",
            "185/185 [==============================] - 9s 51ms/step - loss: 0.0099 - accuracy: 0.2331\n",
            "Epoch 45/50\n",
            "185/185 [==============================] - 9s 51ms/step - loss: 0.0096 - accuracy: 0.2333\n",
            "Epoch 46/50\n",
            "185/185 [==============================] - 9s 48ms/step - loss: 0.0088 - accuracy: 0.2334\n",
            "Epoch 47/50\n",
            "185/185 [==============================] - 9s 47ms/step - loss: 0.0086 - accuracy: 0.2333\n",
            "Epoch 48/50\n",
            "185/185 [==============================] - 9s 48ms/step - loss: 0.0084 - accuracy: 0.2334\n",
            "Epoch 49/50\n",
            "185/185 [==============================] - 9s 48ms/step - loss: 0.0079 - accuracy: 0.2335\n",
            "Epoch 50/50\n",
            "185/185 [==============================] - 9s 51ms/step - loss: 0.0076 - accuracy: 0.2336\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f07fff9a290>"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **4) 챗봇 구현 및 성능 확인**\n",
        "트랜스포머 모델로 챗봇을 만들어 확인해보도록 하겠습니다."
      ],
      "metadata": {
        "id": "teZ8ezy2LgSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(sentence):\n",
        "  sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "  sentence = sentence.strip()\n",
        "  return sentence"
      ],
      "metadata": {
        "id": "JKF20ClTLH17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(sentence):\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  sentence = tf.expand_dims(\n",
        "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
        "\n",
        "  output = tf.expand_dims(START_TOKEN, 0)\n",
        "\n",
        "  # 디코더의 예측 시작\n",
        "  for i in range(MAX_LENGTH):\n",
        "    predictions = model(inputs=[sentence, output], training=False)\n",
        "\n",
        "    # 현재(마지막) 시점의 예측 단어를 받아온다.\n",
        "    predictions = predictions[:, -1:, :]\n",
        "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "\n",
        "    # 만약 마지막 시점의 예측 단어가 종료 토큰이라면 예측을 중단\n",
        "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
        "      break\n",
        "\n",
        "    # 마지막 시점의 예측 단어를 출력에 연결한다.\n",
        "    # 이는 for문을 통해서 디코더의 입력으로 사용될 예정이다.\n",
        "    output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "  return tf.squeeze(output, axis=0)\n",
        "\n",
        "\n",
        "def predict(sentence):\n",
        "  prediction = evaluate(sentence)\n",
        "\n",
        "  predicted_sentence = tokenizer.decode(\n",
        "      [i for i in prediction if i < tokenizer.vocab_size])\n",
        "\n",
        "  return predicted_sentence"
      ],
      "metadata": {
        "id": "SSQ81_EKLFCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#간단한 챗봇 구현 -> q를 누르면 종료됩니다.\n",
        "while True:\n",
        "    user_input = input('대화를 입력하세요\\n')\n",
        "    if user_input =='q':\n",
        "        break\n",
        "    print('answer: {}'.format(predict(user_input)))"
      ],
      "metadata": {
        "id": "q9bQz3l1LMnN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0becdba8-4477-4536-8121-c1054627d049"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "대화를 입력하세요\n",
            "안녕하세요\n",
            "answer: 안녕하세요 .\n",
            "대화를 입력하세요\n",
            "배고파\n",
            "answer: 뭐 좀 챙겨드세요 .\n",
            "대화를 입력하세요\n",
            "나 오늘 아파\n",
            "answer: 맘 고생 많았어요 .\n",
            "대화를 입력하세요\n",
            "q\n"
          ]
        }
      ]
    }
  ]
}